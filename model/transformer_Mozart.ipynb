{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngQefdQBMh-6",
        "outputId": "55bfe4f3-0d5a-4902-c00c-e2e6158d8af5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ì´ MIDI: 536ê°œ\n",
            "mozart í›„ë³´: 536ê°œ\n",
            "ë©”íƒ€ ì—†ìŒìœ¼ë¡œ ìŠ¤í‚µ: 0ê°œ, íŒŒì¼ëª… ê·œì¹™ ë¶ˆì¼ì¹˜ ìŠ¤í‚µ: 0ê°œ\n",
            "\n",
            "=== ë¶„í•  ê²°ê³¼ ===\n",
            "train: 321, validation: 107, test: 108\n",
            "ì €ì¥(JSON): /content/drive/MyDrive/Deep_Learning_project/mozart_dataset_TF/flattened_metadata_with_split.json\n",
            "ì €ì¥(CSV):  /content/drive/MyDrive/Deep_Learning_project/mozart_dataset_TF/flattened_metadata_with_split.csv\n",
            "ì¶œë ¥ í´ë”: /content/drive/MyDrive/Deep_Learning_project/mozart_dataset_TF (train/ validation/ test)\n",
            "\n",
            "(ì°¸ê³ ) optional í•„ë“œ ê²°ì¸¡ ê°œìˆ˜ â†’ {'music_period': 37, 'difficulty': 393, 'opus': 27}\n"
          ]
        }
      ],
      "source": [
        "# === mozartë§Œ 6:2:2ë¡œ ë¶„ë¦¬ + flattened_metadata_with_split.json ìƒì„± ===\n",
        "from pathlib import Path\n",
        "import os, json, re, math, shutil, glob, random\n",
        "\n",
        "# --- ê²½ë¡œ ì„¤ì • --- # ğŸ™‚ğŸ™‚ ê°œë³„ í™˜ê²½ë”°ë¼ ë³€ê²½ ğŸ™‚ğŸ™‚\n",
        "# Google Driveê°€ ë§ˆìš´íŠ¸ëœ ê²½ë¡œ ë° í”„ë¡œì íŠ¸ í´ë” ì„¤ì •\n",
        "PROJ = Path(\"/content/drive/MyDrive/Deep_Learning_project\")\n",
        "\n",
        "# metadata.json íŒŒì¼ ê²½ë¡œ ì„¤ì •\n",
        "META_JSON = PROJ / \"metadata.json\"\n",
        "\n",
        "# MIDI íŒŒì¼ë“¤ì´ ìˆëŠ” ë°ì´í„° ë£¨íŠ¸ í´ë” ì„¤ì •\n",
        "DATA_ROOT = PROJ / \"original_token\" / \"mozart_midis\"\n",
        "\n",
        "\n",
        "SPLIT_ROOT = PROJ / \"mozart_dataset_TF\"\n",
        "SPLIT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ì—¬ëŸ¬ ë²ˆ ì‹¤í–‰ ì‹œ, ì•„ë˜ë¥¼ Trueë¡œ ë‘ë©´ train/validation/test í´ë”ë¥¼ ë¹„ìš°ê³  ì‹œì‘í•©ë‹ˆë‹¤.\n",
        "CLEAN_SPLIT = True\n",
        "if CLEAN_SPLIT:\n",
        "    for sub in [\"train\", \"validation\", \"test\"]:\n",
        "        shutil.rmtree(SPLIT_ROOT / sub, ignore_errors=True)\n",
        "\n",
        "for sub in [\"train\", \"validation\", \"test\"]:\n",
        "    (SPLIT_ROOT / sub).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- ìœ í‹¸ ---\n",
        "def parse_filename(fp: Path):\n",
        "    m = re.match(r\"^(\\d{6})_(\\d+)\\.mid$\", fp.name)\n",
        "    if not m:\n",
        "        return None, None\n",
        "    id_num = int(m.group(1))\n",
        "    take = m.group(2)\n",
        "    return str(id_num), take  # metadata.jsonì˜ í‚¤ëŠ” '4' ê°™ì€ í˜•íƒœ\n",
        "\n",
        "def pick_audio_score(meta_entry: dict, take: str):\n",
        "    aud = meta_entry.get(\"audio_scores\", {})\n",
        "    if isinstance(aud, dict) and aud:\n",
        "        if take in aud:\n",
        "            return aud[take]\n",
        "        try:\n",
        "            return next(iter(aud.values()))\n",
        "        except StopIteration:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def is_mozart(composer_val):  # ğŸ™‚ğŸ™‚ ì‘ê³¡ê°€ëª…ë”°ë¼ ë³€ê²½ ğŸ™‚ğŸ™‚\n",
        "    if composer_val is None:\n",
        "        return False\n",
        "    return \"mozart\" in str(composer_val).lower()  # ğŸ™‚ğŸ™‚ ì‘ê³¡ê°€ëª…ë”°ë¼ ë³€ê²½ ğŸ™‚ğŸ™‚\n",
        "\n",
        "# --- ë©”íƒ€ ë¡œë“œ ---\n",
        "assert META_JSON.exists(), f\"metadata.json not found: {META_JSON}\"\n",
        "with open(META_JSON, \"r\") as f:\n",
        "    meta_raw = json.load(f)\n",
        "\n",
        "# --- ë°ì´í„° ìŠ¤ìº”: ëª¨ë“  .mid íŒŒì¼ ---\n",
        "all_mid_paths = [Path(p) for p in glob.glob(str(DATA_ROOT / \"**\" / \"*.mid\"), recursive=True)]\n",
        "\n",
        "# --- mozartë§Œ í•„í„°ë§ ---  # ğŸ™‚ğŸ™‚ ì‘ê³¡ê°€ëª…ë”°ë¼ ë³€ê²½ ğŸ™‚ğŸ™‚\n",
        "beet_items = []\n",
        "skipped_no_meta = 0\n",
        "skipped_bad_name = 0\n",
        "\n",
        "for fp in all_mid_paths:\n",
        "    id_str, take = parse_filename(fp)\n",
        "    if not id_str:\n",
        "        skipped_bad_name += 1\n",
        "        continue\n",
        "    entry = meta_raw.get(id_str)\n",
        "    if not entry:\n",
        "        skipped_no_meta += 1\n",
        "        continue\n",
        "\n",
        "    md = entry.get(\"metadata\", {})\n",
        "    if not is_mozart(md.get(\"composer\")):  # ğŸ™‚ğŸ™‚ ì‘ê³¡ê°€ëª…ë”°ë¼ ë³€ê²½ ğŸ™‚ğŸ™‚\n",
        "        continue\n",
        "\n",
        "    beet_items.append((fp, id_str, take, entry))\n",
        "\n",
        "print(f\"ì´ MIDI: {len(all_mid_paths)}ê°œ\")\n",
        "print(f\"mozart í›„ë³´: {len(beet_items)}ê°œ\")  # ğŸ™‚ğŸ™‚ ì‘ê³¡ê°€ëª…ë”°ë¼ ë³€ê²½ ğŸ™‚ğŸ™‚\n",
        "print(f\"ë©”íƒ€ ì—†ìŒìœ¼ë¡œ ìŠ¤í‚µ: {skipped_no_meta}ê°œ, íŒŒì¼ëª… ê·œì¹™ ë¶ˆì¼ì¹˜ ìŠ¤í‚µ: {skipped_bad_name}ê°œ\")\n",
        "\n",
        "# --- 6:2:2 ë¶„í•  ---\n",
        "SEED = 42\n",
        "random.Random(SEED).shuffle(beet_items)\n",
        "\n",
        "N = len(beet_items)\n",
        "n_train = math.floor(N * 0.6)\n",
        "n_val   = math.floor(N * 0.2)\n",
        "n_test  = N - n_train - n_val\n",
        "\n",
        "splits = (\n",
        "    [(\"train\", 0.6)] * n_train +\n",
        "    [(\"validation\", 0.2)] * n_val +\n",
        "    [(\"test\", 0.2)] * n_test\n",
        ")\n",
        "\n",
        "# --- ë³µì‚¬ & í”Œë˜íŠ¼ ë©”íƒ€ êµ¬ì„± ---\n",
        "flat_meta = {}  # key: íŒŒì¼ëª…, val: ë©”íƒ€ dict\n",
        "missing_optionals = {\"music_period\": 0, \"difficulty\": 0, \"genre\": 0, \"opus\": 0}\n",
        "\n",
        "for (item, (split_name, split_ratio)) in zip(beet_items, splits):\n",
        "    fp, id_str, take, entry = item\n",
        "    md = entry.get(\"metadata\", {})\n",
        "\n",
        "    basename = fp.name\n",
        "    audio_score = pick_audio_score(entry, take)\n",
        "\n",
        "    music_period = md.get(\"music_period\")\n",
        "    difficulty   = md.get(\"difficulty\")\n",
        "    genre        = md.get(\"genre\")\n",
        "    opus         = md.get(\"opus\")\n",
        "\n",
        "    if music_period is None: missing_optionals[\"music_period\"] += 1\n",
        "    if difficulty   is None: missing_optionals[\"difficulty\"]   += 1\n",
        "    if genre        is None: missing_optionals[\"genre\"]        += 1\n",
        "    if opus         is None: missing_optionals[\"opus\"]         += 1\n",
        "\n",
        "    dst = SPLIT_ROOT / split_name / basename\n",
        "    shutil.copy2(fp, dst)  # ê°™ì€ ì´ë¦„ì´ë©´ ë®ì–´ì”€\n",
        "\n",
        "    flat_meta[basename] = {\n",
        "        \"file_path\": basename,       # ex) 000004_0.mid\n",
        "        \"split\": split_name,         # train / validation / test\n",
        "        \"composer\": md.get(\"composer\"),\n",
        "        \"music_period\": music_period,\n",
        "        \"difficulty\": difficulty,\n",
        "        \"genre\": genre,\n",
        "        \"audio_score\": audio_score,\n",
        "        \"opus\": opus,\n",
        "        \"split_ratio\": split_ratio,\n",
        "    }\n",
        "\n",
        "# --- JSON/CSV ì €ì¥: ì“°ê¸° ê°€ëŠ¥í•œ SPLIT_ROOTì— ì €ì¥ ---\n",
        "OUT_JSON = SPLIT_ROOT / \"flattened_metadata_with_split.json\"\n",
        "OUT_CSV  = SPLIT_ROOT / \"flattened_metadata_with_split.csv\"\n",
        "\n",
        "with open(OUT_JSON, \"w\") as f:\n",
        "    json.dump(flat_meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# CSVë„ ê°™ì´ ì €ì¥(í¸ì˜)\n",
        "import pandas as pd\n",
        "pd.DataFrame.from_dict(flat_meta, orient=\"index\").reset_index(drop=True).to_csv(OUT_CSV, index=False)\n",
        "\n",
        "print(\"\\n=== ë¶„í•  ê²°ê³¼ ===\")\n",
        "print(f\"train: {n_train}, validation: {n_val}, test: {n_test}\")\n",
        "print(f\"ì €ì¥(JSON): {OUT_JSON}\")\n",
        "print(f\"ì €ì¥(CSV):  {OUT_CSV}\")\n",
        "print(f\"ì¶œë ¥ í´ë”: {SPLIT_ROOT} (train/ validation/ test)\")\n",
        "print(\"\\n(ì°¸ê³ ) optional í•„ë“œ ê²°ì¸¡ ê°œìˆ˜ â†’\", {k:v for k,v in missing_optionals.items() if v>0})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks6yyzFEVcEZ",
        "outputId": "42120390-ed02-4e14-c6b2-2c36a7917052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "MPS available: False\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# [1] ëŸ°íƒ€ì„ ì²´í¬ (MPS + CPU)\n",
        "import torch, platform, sys, os, subprocess, textwrap, random, numpy as np\n",
        "\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"MPS available:\", torch.backends.mps.is_available())\n",
        "\n",
        "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# ì¬í˜„ì„±\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if device.type == \"mps\":\n",
        "    pass  # MPSëŠ” ë³„ë„ì˜ manual_seed_all ì—†ìŒ\n",
        "else:\n",
        "    torch.cuda.manual_seed_all(seed)  # í˜¹ì‹œ cuda fallback ë  ê²½ìš°ë§Œ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD9KN_NNQJF7",
        "outputId": "30bba47f-dfc7-48dc-f5ff-725c7415a913",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/5.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/5.6 MB\u001b[0m \u001b[31m168.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/5.6 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”\u001b[0m \u001b[32m5.4/5.6 MB\u001b[0m \u001b[31m54.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mido\n",
            "  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Collecting pyfluidsynth\n",
            "  Downloading pyfluidsynth-1.3.4-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: music21 in /usr/local/lib/python3.12/dist-packages (9.3.0)\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from pretty_midi) (2.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from pretty_midi) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mido) (25.0)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.12/dist-packages (from music21) (5.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from music21) (1.5.2)\n",
            "Requirement already satisfied: jsonpickle in /usr/local/lib/python3.12/dist-packages (from music21) (4.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from music21) (3.10.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from music21) (10.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from music21) (2.32.4)\n",
            "Requirement already satisfied: webcolors>=1.5 in /usr/local/lib/python3.12/dist-packages (from music21) (24.11.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->music21) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->music21) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->music21) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->music21) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->music21) (2025.8.3)\n",
            "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyfluidsynth-1.3.4-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592286 sha256=aebb13f2893ca2d41ab346684e83a4dc870c723865e9a39f5793d25a29dcc99d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/f9/9e/08350c27e386558df0ae234e28a8facd145ba45506ddd1b989\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: pyfluidsynth, mido, pretty_midi\n",
            "Successfully installed mido-1.3.3 pretty_midi-0.2.10 pyfluidsynth-1.3.4\n"
          ]
        }
      ],
      "source": [
        "!{sys.executable} -m pip install pretty_midi mido einops pyfluidsynth music21\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-xZ2WxIR2FZ"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json, random, math, os\n",
        "import numpy as np\n",
        "import pretty_midi as pm\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from einops import rearrange\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SPLIT_META_JSON = SPLIT_ROOT / \"flattened_metadata_with_split.json\"\n",
        "assert SPLIT_META_JSON.exists(), f\"split ë©”íƒ€ê°€ ì—†ìŠµë‹ˆë‹¤: {SPLIT_META_JSON}\"\n",
        "with open(SPLIT_META_JSON, \"r\") as f:\n",
        "    META = json.load(f)\n",
        "\n",
        "# í† í°/ìºì‹œ ê²½ë¡œ\n",
        "VOCAB_JSON   = SPLIT_ROOT / \"vocab.json\"\n",
        "TOK_CACHE_DIR= SPLIT_ROOT / \"tok_cache\"\n",
        "TOK_CACHE_DIR.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We7OVO4KXkSg"
      },
      "source": [
        "### í† í°í™”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9qKqnZKX4FR"
      },
      "outputs": [],
      "source": [
        "from music21 import converter\n",
        "\n",
        "# ìŒì´ë¦„ í‘œê¸°: MIDI pitch class â†’ 'C','C#',...,'B'\n",
        "PC2NAME = ['C','C#','D','Eb','E','F','F#','G','Ab','A','Bb','B']\n",
        "\n",
        "def detect_key_with_music21(midi_path: Path):\n",
        "    \"\"\"music21ë¡œ ì „ì²´ ê³¡ì˜ ì¡°ì„±(ì¥/ë‹¨ì¡°)ì„ ì¶”ì •í•´ KEY í† í°ì„ ëŒë ¤ì¤ë‹ˆë‹¤.\"\"\"\n",
        "    try:\n",
        "        s = converter.parse(str(midi_path))\n",
        "        k = s.analyze('key')\n",
        "        tonic_name = k.tonic.name  # e.g., 'C', 'E-'\n",
        "        # music21ì˜ E- ê°™ì€ í‘œê¸°ë¥¼ ì¢€ ë” ì¼ë°˜ì ìœ¼ë¡œ ë³€í™˜\n",
        "        tonic_name = tonic_name.replace('-','b')  # E- â†’ Eb\n",
        "        mode = 'maj' if k.mode.lower().startswith('maj') else 'min'\n",
        "        return f\"{tonic_name}{'maj' if mode=='maj' else 'min'}\"  # ì˜ˆ: Cmaj, Amin, Ebmaj\n",
        "    except Exception:\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmX3QrI1X7GO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ê¸°ë³¸ í™”ìŒ í…œí”Œë¦¿(ë£¨íŠ¸=0 ê¸°ì¤€ì˜ pitch-class ì§‘í•©)\n",
        "CHORD_TEMPLATES = {\n",
        "    'maj'       : {0,4,7},\n",
        "    'min'       : {0,3,7},\n",
        "    'dim'       : {0,3,6},\n",
        "    'aug'       : {0,4,8},\n",
        "    'dom7'      : {0,4,7,10},\n",
        "    'maj7'      : {0,4,7,11},\n",
        "    'min7'      : {0,3,7,10},\n",
        "    'halfdim7'  : {0,3,6,10},\n",
        "    'dim7'      : {0,3,6,9},\n",
        "}\n",
        "\n",
        "def best_chord_label(pitches):\n",
        "    \"\"\"\n",
        "    pitches: ë¦¬ìŠ¤íŠ¸/ì…‹ (MIDI pitchë“¤) â†’ ìµœì ì˜ (root, quality) ë°˜í™˜. ì—†ìœ¼ë©´ None.\n",
        "    í‰ê°€ ê¸°ì¤€: í…œí”Œë¦¿ ì»¤ë²„ ë¹„ìœ¨ + ì‚¬ì´ì¦ˆ ê·¼ì‚¬ì„±.\n",
        "    \"\"\"\n",
        "    if not pitches:\n",
        "        return None\n",
        "    pcs = sorted({p % 12 for p in pitches})\n",
        "    if not pcs:\n",
        "        return None\n",
        "\n",
        "    best = None\n",
        "    best_score = -1e9\n",
        "    for root in range(12):\n",
        "        shifted = {(pc - root) % 12 for pc in pcs}\n",
        "        for qual, templ in CHORD_TEMPLATES.items():\n",
        "            inter = len(shifted & templ)\n",
        "            # í…œí”Œë¦¿ í¬í•¨ ë¹„ìœ¨ & ì—¬ë¶„ íŒ¨ë„í‹°\n",
        "            cover = inter / max(1, len(templ))\n",
        "            extra_penalty = -0.15 * max(0, len(shifted - templ))\n",
        "            score = cover + extra_penalty\n",
        "            if score > best_score and inter >= 2:  # ìµœì†Œ 2ìŒ ì´ìƒ ë§ì•„ì•¼ í™”ìŒìœ¼ë¡œ ì¸ì •\n",
        "                best_score = score\n",
        "                best = (root, qual)\n",
        "    if best is None:\n",
        "        return None\n",
        "    root_name = PC2NAME[best[0]]\n",
        "    return f\"{root_name}:{best[1]}\"  # ì˜ˆ: C:maj, A:min, G:dom7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eepKPSdCX_W3"
      },
      "outputs": [],
      "source": [
        "TS_DIV = 16\n",
        "MIN_VEL, MAX_VEL = 20, 100\n",
        "MIN_DUR, MAX_DUR = 1, 16\n",
        "\n",
        "def quantize_time(pm_obj, ts_div=TS_DIV):\n",
        "    ts = pm_obj.time_signature_changes or [pm.TimeSignature(4,4,0.0)]\n",
        "    tempo_times, tempi = pm_obj.get_tempo_changes()\n",
        "    tempo = float(tempi[0]) if len(tempi) else 120.0\n",
        "    num, den = ts[0].numerator, ts[0].denominator\n",
        "    beat_len = 60.0 / tempo\n",
        "    bar_sec = (4/den) * num * beat_len\n",
        "\n",
        "    notes=[]\n",
        "    for inst in pm_obj.instruments:\n",
        "        for n in inst.notes:\n",
        "            bar = int(n.start // bar_sec)\n",
        "            pos = int(((n.start - bar*bar_sec)/bar_sec)*ts_div); pos = max(0,min(ts_div-1,pos))\n",
        "            dur = int(((n.end - n.start)/bar_sec)*ts_div); dur = max(MIN_DUR, min(MAX_DUR, dur))\n",
        "            vel = int(np.clip(n.velocity, MIN_VEL, MAX_VEL))\n",
        "            notes.append((bar,pos,n.pitch,dur,vel))\n",
        "    notes.sort(key=lambda x:(x[0],x[1],x[2]))\n",
        "    return notes, (num,den), int(tempo), bar_sec\n",
        "\n",
        "def encode_remi_harmony(midi_path: Path, add_chords=True, chord_every='pos'):\n",
        "    \"\"\"\n",
        "    chord_every: 'bar' â†’ ë°” ë‹¹ 1ê°œ, 'beat' â†’ ë°•ì ë‹¨ìœ„(ê·¼ì‚¬), 'pos' â†’ POS ë‹¨ìœ„(onset ê¸°ì¤€).\n",
        "    \"\"\"\n",
        "    pm_obj = pm.PrettyMIDI(str(midi_path))\n",
        "    notes,(num,den),tempo,bar_sec = quantize_time(pm_obj)\n",
        "\n",
        "    # 0) KEY(ì¥/ë‹¨ì¡°) í† í°\n",
        "    key_token = detect_key_with_music21(midi_path)  # ì˜ˆ: 'Cmaj', 'Amin', None\n",
        "\n",
        "    toks = []\n",
        "    toks.append(f\"TSig_{num}_{den}\")\n",
        "    toks.append(f\"TEMPO_{tempo}\")\n",
        "    if key_token:\n",
        "        toks.append(f\"KEY_{key_token}\")  # ì˜ˆ: KEY_Cmaj\n",
        "\n",
        "    # 1) í™”ìŒ ë¼ë²¨ë§ì„ ìœ„í•œ ê·¸ë£¹í•‘\n",
        "    #    - chord_every == 'bar' : ê°™ì€ bar ë‚´ ëª¨ë“  ë…¸íŠ¸ onsets\n",
        "    #    - chord_every == 'pos' : (bar,pos) ë‹¨ìœ„ ë…¸íŠ¸ onsets\n",
        "    #    - chord_every == 'beat': bar ë‚´ beat ê²½ê³„ ê·¼ì‚¬ (num ê°œ)\n",
        "    from collections import defaultdict\n",
        "    onset_map = defaultdict(list)  # key: (bar, pos or beatIndex) â†’ pitches\n",
        "\n",
        "    if chord_every == 'bar':\n",
        "        for (bar,pos,pitch,dur,vel) in notes:\n",
        "            onset_map[(bar, -1)].append(pitch)\n",
        "\n",
        "    elif chord_every == 'beat':\n",
        "        # ë°•ì ê²½ê³„(ëŒ€ëµ)ë¡œ posâ†’beat index ë§¤í•‘ (TS_DIVë¥¼ numë¡œ ë‚˜ëˆ”)\n",
        "        step_per_beat = max(1, TS_DIV // num)\n",
        "        for (bar,pos,pitch,dur,vel) in notes:\n",
        "            beat_idx = pos // step_per_beat\n",
        "            onset_map[(bar, beat_idx)].append(pitch)\n",
        "\n",
        "    else:  # 'pos'\n",
        "        for (bar,pos,pitch,dur,vel) in notes:\n",
        "            onset_map[(bar, pos)].append(pitch)\n",
        "\n",
        "    # 2) í† í° ì‹œí€€ìŠ¤ ìƒì„±\n",
        "    cur_bar = -1\n",
        "    last_chord = None\n",
        "    for (bar,pos,pitch,dur,vel) in notes:\n",
        "        # BAR í† í°\n",
        "        while cur_bar < bar:\n",
        "            toks.append(\"BAR\")\n",
        "            cur_bar += 1\n",
        "            last_chord = None  # ìƒˆ ë§ˆë””ì—ì„œ í™”ìŒ ìƒˆë¡œ íŒë‹¨\n",
        "\n",
        "        # POS í† í°\n",
        "        toks.append(f\"POS_{pos}\")\n",
        "\n",
        "        # 2-a) í™”ìŒ í† í°(ì„ íƒ)\n",
        "        if add_chords:\n",
        "            key = (bar, -1) if chord_every=='bar' else ((bar, pos) if chord_every=='pos' else (bar, pos // max(1, TS_DIV // num)))\n",
        "            chord_label = best_chord_label(onset_map.get(key, []))\n",
        "            if chord_label and chord_label != last_chord:\n",
        "                toks.append(f\"CHORD_{chord_label}\")   # ì˜ˆ: CHORD_C:maj, CHORD_A:min\n",
        "                last_chord = chord_label\n",
        "\n",
        "        # 2-b) ìŒí‘œ ì´ë²¤íŠ¸\n",
        "        toks += [f\"NOTE_ON_{pitch}\", f\"DUR_{dur}\", f\"VEL_{vel}\"]\n",
        "\n",
        "    return toks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oUhK5vZCYD4N"
      },
      "outputs": [],
      "source": [
        "def cond_tokens(meta, midi_path_for_key: Path = None):\n",
        "    t = [\"COMPOSER_Beethoven\"]\n",
        "    if meta.get(\"music_period\"): t.append(f\"PERIOD_{meta['music_period']}\")\n",
        "    if meta.get(\"genre\"):        t.append(f\"GENRE_{meta['genre']}\")\n",
        "    if meta.get(\"difficulty\"):   t.append(f\"DIFF_{meta['difficulty']}\")\n",
        "    if meta.get(\"opus\"):         t.append(f\"OPUS_{str(meta['opus']).replace(' ','_')}\")\n",
        "    q=meta.get(\"audio_score\")\n",
        "    if q is not None:\n",
        "        t.append(f\"QUALITY_{'High' if q>=0.8 else 'Med' if q>=0.5 else 'Low'}\")\n",
        "    # KEY (ë©”íƒ€/íŒŒì¼ ê¸°ë°˜)\n",
        "    if midi_path_for_key is not None:\n",
        "        k = detect_key_with_music21(midi_path_for_key)\n",
        "        if k:\n",
        "            t.append(f\"KEY_{k}\")  # KEY_Cmaj / KEY_Amin ...\n",
        "    return t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g9y46PUSQ4U",
        "outputId": "5fd3ec82-022f-40d0-fa04-03c6720e8508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VOCAB_SIZE: 471\n"
          ]
        }
      ],
      "source": [
        "def build_vocab():\n",
        "    if VOCAB_JSON.exists(): return json.load(open(VOCAB_JSON))\n",
        "    vocab={\"[PAD]\":0,\"[BOS]\":1,\"[EOS]\":2,\"[UNK]\":3}\n",
        "    idx=len(vocab)\n",
        "    for fname,meta in META.items():\n",
        "        p = SPLIT_ROOT/meta[\"split\"]/meta[\"file_path\"]\n",
        "        if not p.exists(): continue\n",
        "        # vocab ë¹Œë“œ ë‹¨ê³„\n",
        "        toks = [\"[BOS]\"] + cond_tokens(meta, midi_path_for_key=p) + encode_remi_harmony(p, add_chords=True, chord_every='pos')+ [\"[EOS]\"]\n",
        "\n",
        "        json.dump(toks, open(TOK_CACHE_DIR/(fname+\".json\"),\"w\"))\n",
        "        for t in toks:\n",
        "            if t not in vocab:\n",
        "                vocab[t]=idx; idx+=1\n",
        "    json.dump(vocab, open(VOCAB_JSON,\"w\"))\n",
        "    return vocab\n",
        "\n",
        "VOCAB = build_vocab()\n",
        "IVOCAB= {i:t for t,i in VOCAB.items()}\n",
        "VOCAB_SIZE=len(VOCAB)\n",
        "print(\"VOCAB_SIZE:\", VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNqiWgG6SUWq"
      },
      "outputs": [],
      "source": [
        "SEQ_LEN=512\n",
        "\n",
        "def toks_to_ids(toks): return [VOCAB.get(t, VOCAB[\"[UNK]\"]) for t in toks]\n",
        "\n",
        "class BeethovenMIDIDataset(Dataset):\n",
        "    def __init__(self, split):\n",
        "        self.items=[(k,v) for k,v in META.items() if v[\"split\"]==split]\n",
        "        random.Random(42).shuffle(self.items)\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self,i):\n",
        "        fname,meta = self.items[i]\n",
        "        cache=TOK_CACHE_DIR/(fname+\".json\")\n",
        "        if cache.exists(): toks=json.load(open(cache))\n",
        "        else:\n",
        "            p=SPLIT_ROOT/meta[\"split\"]/meta[\"file_path\"]\n",
        "            toks=[\"[BOS]\"]+cond_tokens(meta)+encode_remi_lite(p)+[\"[EOS]\"]\n",
        "        ids=toks_to_ids(toks)\n",
        "        if len(ids)>=SEQ_LEN:\n",
        "            st=random.randint(0, len(ids)-SEQ_LEN)\n",
        "            seq=ids[st:st+SEQ_LEN]\n",
        "        else:\n",
        "            seq=ids+[VOCAB[\"[PAD]\"]] * (SEQ_LEN-len(ids))\n",
        "        x=torch.tensor(seq[:-1],dtype=torch.long)\n",
        "        y=torch.tensor(seq[1:], dtype=torch.long)\n",
        "        return x,y\n",
        "\n",
        "train_dl=DataLoader(BeethovenMIDIDataset(\"train\"), batch_size=16, shuffle=True, drop_last=True)\n",
        "val_dl  =DataLoader(BeethovenMIDIDataset(\"validation\"), batch_size=16, shuffle=False, drop_last=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O_BC3kuQSf6q"
      },
      "outputs": [],
      "source": [
        "import math, torch, torch.nn as nn\n",
        "from einops import rearrange\n",
        "\n",
        "# ===== 1) RoPE (Rotary Positional Embedding) =====\n",
        "def apply_rope(q,k):\n",
        "    B,H,T,D = q.shape\n",
        "    pos = torch.arange(T, device=q.device).float()\n",
        "    inv = 1.0/(10000**(torch.arange(0,D,2,device=q.device).float()/D))\n",
        "    ang = torch.einsum('t,d->td', pos, inv)\n",
        "    sin,cos = ang.sin()[None,None], ang.cos()[None,None]\n",
        "    def rot(x):\n",
        "        x1,x2=x[...,::2],x[...,1::2]\n",
        "        return torch.stack([x1*cos - x2*sin, x1*sin + x2*cos], dim=-1).flatten(-2)\n",
        "    return rot(q), rot(k)\n",
        "\n",
        "# ===== 2) Self-Attention =====\n",
        "class CausalSelfAttn(nn.Module):\n",
        "    def __init__(self,d_model=512,n_head=8,p=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model% n_head==0\n",
        "        self.nh=n_head; self.dk=d_model//n_head\n",
        "        self.qkv=nn.Linear(d_model, d_model*3)\n",
        "        self.proj=nn.Linear(d_model, d_model)\n",
        "        self.drop=nn.Dropout(p)\n",
        "    def forward(self,x):\n",
        "        B,T,C=x.shape\n",
        "        q,k,v = self.qkv(x).chunk(3,-1)\n",
        "        q=rearrange(q,'b t (h d)->b h t d',h=self.nh)\n",
        "        k=rearrange(k,'b t (h d)->b h t d',h=self.nh)\n",
        "        v=rearrange(v,'b t (h d)->b h t d',h=self.nh)\n",
        "        q,k=apply_rope(q,k)\n",
        "        att=(q@k.transpose(-1,-2))/math.sqrt(self.dk)\n",
        "        mask=torch.triu(torch.ones(T,T,device=x.device),1).bool()\n",
        "        att=att.masked_fill(mask,float('-inf')).softmax(-1)\n",
        "        att=self.drop(att)\n",
        "        y=att@v\n",
        "        y=rearrange(y,'b h t d->b t (h d)')\n",
        "        return self.proj(y)\n",
        "\n",
        "# ===== 3) Transformer Block =====\n",
        "class Block(nn.Module):\n",
        "    def __init__(self,d=512,h=8,p=0.1,mlp=4):\n",
        "        super().__init__()\n",
        "        self.ln1=nn.LayerNorm(d); self.att=CausalSelfAttn(d,h,p)\n",
        "        self.ln2=nn.LayerNorm(d)\n",
        "        self.mlp=nn.Sequential(\n",
        "            nn.Linear(d,d*mlp), nn.GELU(), nn.Dropout(p), nn.Linear(d*mlp,d)\n",
        "        )\n",
        "        self.drop=nn.Dropout(p)\n",
        "    def forward(self,x):\n",
        "        x=x+self.drop(self.att(self.ln1(x)))\n",
        "        x=x+self.drop(self.mlp(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "# ===== 4) MiniGPT Model =====\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self,vocab_size, d=512, L=8, H=8, p=0.1):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(vocab_size, d)\n",
        "        self.pos=nn.Parameter(torch.zeros(1, SEQ_LEN-1, d))  # ì ˆëŒ€ ìœ„ì¹˜ (RoPEì™€ ë³‘ìš©)\n",
        "        self.blocks=nn.ModuleList([Block(d,H,p) for _ in range(L)])\n",
        "        self.ln=nn.LayerNorm(d)\n",
        "        self.head=nn.Linear(d, vocab_size, bias=False)\n",
        "    def forward(self,idx):\n",
        "        x=self.emb(idx) + self.pos[:, :idx.size(1), :]\n",
        "        for b in self.blocks: x=b(x)\n",
        "        return self.head(self.ln(x))\n",
        "\n",
        "# ===== 5) ëª¨ë¸ ì´ˆê¸°í™” =====\n",
        "model = MiniGPT(VOCAB_SIZE).to(DEVICE)\n",
        "opt   = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9,0.95), weight_decay=0.1)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=VOCAB[\"[PAD]\"], label_smoothing=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWRsoGe7apIg"
      },
      "outputs": [],
      "source": [
        "import pretty_midi as pm\n",
        "from pathlib import Path\n",
        "\n",
        "def detokenize_to_midi(tokens, out_path, ts_div=16, default_time_sig=(4,4), default_tempo=120,\n",
        "                       program=0, safe_pitch=(36,96)):\n",
        "    \"\"\"\n",
        "    REMIë¥˜ í† í° ì‹œí€€ìŠ¤ë¥¼ MIDIë¡œ ë³µì›í•©ë‹ˆë‹¤.\n",
        "    - tokens ì˜ˆì‹œ: [\"TSig_4_4\",\"TEMPO_112\",\"BAR\",\"POS_0\",\"NOTE_ON_60\",\"DUR_4\",\"VEL_80\", ...]\n",
        "    - KEY_*, CHORD_* í† í°ì€ MIDIì— ì§ì ‘ ë°˜ì˜í•˜ì§€ ì•Šê³  ê±´ë„ˆëœë‹ˆë‹¤.\n",
        "    - ts_divëŠ” í† í°í™” ë•Œ ì“´ TS_DIVì™€ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤(ê¸°ë³¸ 16).\n",
        "    \"\"\"\n",
        "    out_path = Path(out_path) if isinstance(out_path, str) else out_path\n",
        "\n",
        "    # ì´ˆê¸° ë©”íƒ€\n",
        "    num, den = default_time_sig\n",
        "    tempo = default_tempo\n",
        "    beat_len = 60.0 / tempo\n",
        "    bar_sec = (4/den) * num * beat_len\n",
        "\n",
        "    cur_bar = -1\n",
        "    pos = 0\n",
        "\n",
        "    pm_out = pm.PrettyMIDI()\n",
        "    inst = pm.Instrument(program=program)\n",
        "    SAFE_LOW, SAFE_HIGH = safe_pitch\n",
        "\n",
        "    i = 0\n",
        "    N = len(tokens)\n",
        "    while i < N:\n",
        "        t = tokens[i]\n",
        "\n",
        "        # ë©”íƒ€ í† í°\n",
        "        if t.startswith(\"TSig_\"):\n",
        "            try:\n",
        "                _, a, b = t.split(\"_\")\n",
        "                num, den = int(a), int(b)\n",
        "                beat_len = 60.0 / tempo\n",
        "                bar_sec = (4/den) * num * beat_len\n",
        "            except Exception:\n",
        "                pass\n",
        "            i += 1; continue\n",
        "\n",
        "        if t.startswith(\"TEMPO_\"):\n",
        "            try:\n",
        "                tempo = int(t.split(\"_\")[1])\n",
        "                beat_len = 60.0 / tempo\n",
        "                bar_sec = (4/den) * num * beat_len\n",
        "            except Exception:\n",
        "                pass\n",
        "            i += 1; continue\n",
        "\n",
        "        if t.startswith(\"KEY_\") or t.startswith(\"CHORD_\"):\n",
        "            i += 1; continue  # ì¡°ì„±/í™”ìŒ í† í°ì€ MIDIì— ì§ì ‘ ë°˜ì˜í•˜ì§€ ì•ŠìŒ\n",
        "\n",
        "        # êµ¬ì¡° í† í°\n",
        "        if t == \"BAR\":\n",
        "            cur_bar += 1\n",
        "            pos = 0\n",
        "            i += 1; continue\n",
        "\n",
        "        if t.startswith(\"POS_\"):\n",
        "            try:\n",
        "                pos = int(t.split(\"_\")[1])\n",
        "                pos = max(0, min(ts_div-1, pos))\n",
        "            except Exception:\n",
        "                pos = max(0, min(ts_div-1, pos))\n",
        "            i += 1; continue\n",
        "\n",
        "        # ìŒí‘œ ì´ë²¤íŠ¸\n",
        "        if t.startswith(\"NOTE_ON_\"):\n",
        "            # NOTE_ON_p\n",
        "            try:\n",
        "                pitch = int(t.split(\"_\")[2])\n",
        "            except Exception:\n",
        "                i += 1; continue\n",
        "            i += 1\n",
        "\n",
        "            # DUR_d (ê¸°ë³¸ 4), VEL_v (ê¸°ë³¸ 80)\n",
        "            dur = 4\n",
        "            vel = 80\n",
        "            if i < N and tokens[i].startswith(\"DUR_\"):\n",
        "                try: dur = int(tokens[i].split(\"_\")[1])\n",
        "                except Exception: pass\n",
        "                i += 1\n",
        "            if i < N and (tokens[i].startswith(\"VEL_\") or tokens[i].startswith(\"VELOCITY_\")):\n",
        "                try: vel = int(tokens[i].split(\"_\")[1])\n",
        "                except Exception: pass\n",
        "                i += 1\n",
        "\n",
        "            pitch = max(SAFE_LOW, min(SAFE_HIGH, pitch))\n",
        "            start = (cur_bar * bar_sec) + (pos / ts_div) * bar_sec\n",
        "            end   = start + (dur / ts_div) * bar_sec\n",
        "            if end <= start:\n",
        "                end = start + (1 / ts_div) * bar_sec  # ìµœì†Œ 1í‹±\n",
        "\n",
        "            inst.notes.append(pm.Note(velocity=vel, pitch=pitch, start=start, end=end))\n",
        "            continue\n",
        "\n",
        "        # ì•Œ ìˆ˜ ì—†ëŠ” í† í°ì€ ë¬´ì‹œ\n",
        "        i += 1\n",
        "\n",
        "    pm_out.instruments.append(inst)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    pm_out.write(str(out_path))\n",
        "    return out_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbeE1fjcZg71",
        "outputId": "c79f7734-836c-4751-9ff8-dd79ba734ff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/50] train 6.3653 | val 6.3803 | best 6.3803 | lr 0.000003\n",
            "[2/50] train 6.3194 | val 6.3728 | best 6.3728 | lr 0.000006\n",
            "[3/50] train 6.2248 | val 6.3799 | best 6.3728 | lr 0.000009\n",
            "[4/50] train 6.0734 | val 6.3722 | best 6.3722 | lr 0.000012\n",
            "[5/50] train 5.8805 | val 6.3636 | best 6.3636 | lr 0.000015\n",
            "[6/50] train 5.6667 | val 6.3607 | best 6.3607 | lr 0.000018\n",
            "[7/50] train 5.4336 | val 6.3488 | best 6.3488 | lr 0.000021\n",
            "[8/50] train 5.2518 | val 6.3348 | best 6.3348 | lr 0.000024\n",
            "[9/50] train 5.0532 | val 6.3199 | best 6.3199 | lr 0.000027\n",
            "[10/50] train 4.8896 | val 6.3068 | best 6.3068 | lr 0.000030\n",
            "[11/50] train 4.7120 | val 6.2826 | best 6.2826 | lr 0.000033\n",
            "[12/50] train 4.5868 | val 6.2588 | best 6.2588 | lr 0.000036\n",
            "[13/50] train 4.4825 | val 6.2335 | best 6.2335 | lr 0.000039\n",
            "[14/50] train 4.3387 | val 6.2103 | best 6.2103 | lr 0.000042\n",
            "[15/50] train 4.1987 | val 6.1779 | best 6.1779 | lr 0.000045\n",
            "[16/50] train 4.0673 | val 6.1568 | best 6.1568 | lr 0.000048\n",
            "[17/50] train 3.9405 | val 6.1265 | best 6.1265 | lr 0.000051\n",
            "[18/50] train 3.8288 | val 6.0947 | best 6.0947 | lr 0.000054\n",
            "[19/50] train 3.7023 | val 6.0727 | best 6.0727 | lr 0.000057\n",
            "[20/50] train 3.5815 | val 6.0352 | best 6.0352 | lr 0.000060\n",
            "[21/50] train 3.4842 | val 6.0055 | best 6.0055 | lr 0.000063\n",
            "[22/50] train 3.4117 | val 5.9679 | best 5.9679 | lr 0.000066\n",
            "[23/50] train 3.3309 | val 5.9402 | best 5.9402 | lr 0.000069\n",
            "[24/50] train 3.2740 | val 5.8938 | best 5.8938 | lr 0.000072\n",
            "[25/50] train 3.2291 | val 5.8695 | best 5.8695 | lr 0.000075\n",
            "[26/50] train 3.1990 | val 5.8200 | best 5.8200 | lr 0.000078\n",
            "[27/50] train 3.1681 | val 5.8075 | best 5.8075 | lr 0.000081\n",
            "[28/50] train 3.1540 | val 5.7512 | best 5.7512 | lr 0.000084\n",
            "[29/50] train 3.1289 | val 5.7042 | best 5.7042 | lr 0.000087\n",
            "[30/50] train 3.1168 | val 5.6647 | best 5.6647 | lr 0.000090\n",
            "[31/50] train 3.0955 | val 5.6200 | best 5.6200 | lr 0.000093\n",
            "[32/50] train 3.0922 | val 5.5956 | best 5.5956 | lr 0.000096\n",
            "[33/50] train 3.0902 | val 5.5521 | best 5.5521 | lr 0.000099\n",
            "[34/50] train 3.0626 | val 5.5201 | best 5.5201 | lr 0.000102\n",
            "[35/50] train 3.0272 | val 5.4711 | best 5.4711 | lr 0.000105\n",
            "[36/50] train 3.0556 | val 5.4277 | best 5.4277 | lr 0.000108\n",
            "[37/50] train 3.0486 | val 5.3725 | best 5.3725 | lr 0.000111\n",
            "[38/50] train 3.0342 | val 5.3561 | best 5.3561 | lr 0.000114\n",
            "[39/50] train 3.0183 | val 5.2979 | best 5.2979 | lr 0.000117\n",
            "[40/50] train 3.0139 | val 5.2499 | best 5.2499 | lr 0.000120\n",
            "[41/50] train 3.0085 | val 5.2123 | best 5.2123 | lr 0.000123\n",
            "[42/50] train 3.0150 | val 5.1704 | best 5.1704 | lr 0.000126\n",
            "[43/50] train 2.9995 | val 5.1407 | best 5.1407 | lr 0.000129\n",
            "[44/50] train 3.0045 | val 5.0975 | best 5.0975 | lr 0.000132\n",
            "[45/50] train 2.9980 | val 5.0645 | best 5.0645 | lr 0.000135\n",
            "[46/50] train 2.9760 | val 5.0083 | best 5.0083 | lr 0.000138\n",
            "[47/50] train 2.9795 | val 4.9892 | best 4.9892 | lr 0.000141\n",
            "[48/50] train 2.9704 | val 4.9184 | best 4.9184 | lr 0.000144\n",
            "[49/50] train 2.9645 | val 4.8820 | best 4.8820 | lr 0.000147\n",
            "[50/50] train 2.9601 | val 4.8705 | best 4.8705 | lr 0.000150\n",
            "Saved â†’ /content/drive/MyDrive/Deep_Learning_project/mozart_dataset_TF/sample_beethoven_long_v1.mid\n",
            "Best val loss: 4.870522499084473 | approx PPL: 130.3890272484736\n"
          ]
        }
      ],
      "source": [
        "# ====== 0) ì¤€ë¹„ ======\n",
        "import math, torch\n",
        "import torch.nn as nn\n",
        "from torch.amp import GradScaler, autocast  # âœ… AMP ìµœì‹  API\n",
        "\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "\n",
        "# ====== EMA ìœ í‹¸ ======\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.decay = decay\n",
        "        self.shadow = {k: p.detach().clone() for k, p in model.state_dict().items()}\n",
        "        self.backup = None\n",
        "    @torch.no_grad()\n",
        "    def update(self, model):\n",
        "        for k, p in model.state_dict().items():\n",
        "            self.shadow[k].mul_(self.decay).add_(p.detach(), alpha=1 - self.decay)\n",
        "    def store(self, model):\n",
        "        self.backup = {k: p.detach().clone() for k, p in model.state_dict().items()}\n",
        "    def copy_to(self, model):\n",
        "        model.load_state_dict(self.shadow, strict=False)\n",
        "    def restore(self, model):\n",
        "        if self.backup is not None:\n",
        "            model.load_state_dict(self.backup, strict=False)\n",
        "            self.backup = None\n",
        "\n",
        "ema = EMA(model, decay=0.999)\n",
        "\n",
        "# ====== 1) ì†ì‹¤, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬, ì²´í¬í¬ì¸íŠ¸ ======\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    ignore_index=VOCAB[\"[PAD]\"],\n",
        "    label_smoothing=0.05,    # âœ… 0.05ë¡œ ë¯¸ì„¸ì¡°ì •\n",
        ")\n",
        "\n",
        "opt = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=3e-4,\n",
        "    betas=(0.9, 0.95),\n",
        "    weight_decay=0.03,       # âœ… ê³¼ê·œì œ ì™„í™”\n",
        ")\n",
        "\n",
        "EPOCHS = 50\n",
        "warmup_steps = 1000\n",
        "total_steps  = max(1, len(train_dl) * EPOCHS)\n",
        "\n",
        "def lr_lambda(step):\n",
        "    if step < warmup_steps:\n",
        "        return step / max(1, warmup_steps)\n",
        "    progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "    progress = min(1.0, max(0.0, progress))\n",
        "    return 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "sched = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
        "\n",
        "use_cuda_amp = (DEVICE.type == 'cuda')\n",
        "scaler = GradScaler('cuda' if use_cuda_amp else 'cpu')\n",
        "\n",
        "# ====== 2) í•™ìŠµ/ê²€ì¦ í•¨ìˆ˜ (ACC=2) ======\n",
        "ACC = 2  # âœ… ìœ íš¨ ë°°ì¹˜ x2\n",
        "\n",
        "def run_epoch(dl, train=True, grad_clip=1.0):\n",
        "    model.train(train)\n",
        "    total, n = 0.0, 0\n",
        "    if train:\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "    for step, (x, y) in enumerate(dl, 1):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        with torch.set_grad_enabled(train):\n",
        "            with autocast('cuda' if use_cuda_amp else 'cpu'):\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits.view(-1, VOCAB_SIZE), y.view(-1))\n",
        "                if train and ACC > 1:\n",
        "                    loss = loss / ACC\n",
        "        if train:\n",
        "            scaler.scale(loss).backward()\n",
        "            if step % ACC == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "                scaler.step(opt)\n",
        "                scaler.update()\n",
        "                sched.step()\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                ema.update(model)  # âœ… step ë’¤ EMA ì—…ë°ì´íŠ¸\n",
        "        total += loss.item() * (ACC if train and ACC > 1 else 1.0)\n",
        "        n += 1\n",
        "    return total / max(1, n)\n",
        "\n",
        "# ====== 3) í•™ìŠµ ë£¨í”„ + EMA ê²€ì¦ + ë² ìŠ¤íŠ¸ ì €ì¥ ======\n",
        "best_val = float('inf')\n",
        "ckpt_path = SPLIT_ROOT / \"minigpt_best.pt\"\n",
        "\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr = run_epoch(train_dl, train=True)\n",
        "\n",
        "    # âœ… EMA ê°€ì¤‘ì¹˜ë¡œ ê²€ì¦\n",
        "    ema.store(model); ema.copy_to(model)\n",
        "    vl = run_epoch(val_dl, train=False)\n",
        "    ema.restore(model)\n",
        "\n",
        "    if vl < best_val:\n",
        "        best_val = vl\n",
        "        torch.save(model.state_dict(), str(ckpt_path))\n",
        "    print(f\"[{ep}/{EPOCHS}] train {tr:.4f} | val {vl:.4f} | best {best_val:.4f} | lr {sched.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "# ====== 4) ë² ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ ë¡œë“œ(ì•ˆì „) ======\n",
        "_ = model.load_state_dict(torch.load(str(ckpt_path), map_location=DEVICE))\n",
        "\n",
        "# ====== 5) ê¸°ë³¸ generate (ìœˆë„ìš° ìë™ ì ìš©: pos ì„ë² ë”© ê¸¸ì´ì— ë§ì¶° crop) ======\n",
        "@torch.no_grad()\n",
        "def generate(prompt_tokens, max_new=700, top_k=50, top_p=0.95, temp=0.9):\n",
        "    model.eval()\n",
        "    unk_id = VOCAB.get(\"[UNK]\", None)\n",
        "    if unk_id is None:\n",
        "        raise ValueError(\"Vocab must contain [UNK] token.\")\n",
        "    pos_len = getattr(getattr(model, 'pos', None), 'shape', [1, 100000, 0])[1]\n",
        "    ids = torch.tensor([[VOCAB.get(t, unk_id) for t in prompt_tokens]], device=DEVICE)\n",
        "    for _ in range(max_new):\n",
        "        ids_win = ids[:, -pos_len:] if ids.size(1) > pos_len else ids\n",
        "        logits = model(ids_win)[:, -1, :] / max(temp, 1e-6)\n",
        "        probs = torch.softmax(logits, dim=-1)[0]\n",
        "        if top_k > 0:\n",
        "            topk = torch.topk(probs, top_k)\n",
        "            mask = torch.ones_like(probs, dtype=torch.bool); mask[topk.indices] = False\n",
        "            probs = probs.masked_fill(mask, 0)\n",
        "        if top_p < 1.0:\n",
        "            sprob, sidx = torch.sort(probs, descending=True)\n",
        "            keep = torch.cumsum(sprob, dim=-1) <= top_p\n",
        "            keep[0] = True\n",
        "            mask = torch.ones_like(probs, dtype=torch.bool); mask[sidx[keep]] = False\n",
        "            probs = probs.masked_fill(mask, 0)\n",
        "        probs = probs / probs.sum()\n",
        "        nxt = torch.multinomial(probs, 1)\n",
        "        ids = torch.cat([ids, nxt.view(1,1)], dim=1)\n",
        "        if nxt.item() == VOCAB[\"[EOS]\"]:\n",
        "            break\n",
        "    return [IVOCAB[i.item()] for i in ids[0]]\n",
        "\n",
        "# ====== 6) ê¸¸ê²Œ ìƒì„±: ì²­í¬ ìŠ¤í‹°ì¹­(ëª¨ë¸ ìˆ˜ì • ì—†ìŒ) ======\n",
        "def stitch_generate(prompt_tokens, total_new=512, chunk_new=700, context=480,\n",
        "                    top_k=50, top_p=0.95, temp=0.9, stop_on_eos=False):\n",
        "    all_tokens = list(prompt_tokens)\n",
        "    made = 0\n",
        "    while made < total_new:\n",
        "        this_new = min(chunk_new, total_new - made)\n",
        "        cur_prompt = all_tokens[-context:] if len(all_tokens) > context else all_tokens\n",
        "        chunk = generate(cur_prompt, max_new=this_new, top_k=top_k, top_p=top_p, temp=temp)\n",
        "        new_part = chunk[len(cur_prompt):] if len(chunk) > len(cur_prompt) else []\n",
        "        if stop_on_eos and (\"[EOS]\" in new_part):\n",
        "            eos_idx = new_part.index(\"[EOS]\"); all_tokens += new_part[:eos_idx]; break\n",
        "        all_tokens += new_part\n",
        "        made += len(new_part)\n",
        "        if len(new_part) == 0:\n",
        "            break\n",
        "    return all_tokens\n",
        "\n",
        "# ====== 7) í”„ë¡¬í”„íŠ¸ ì„¤ì • & ê¸¸ê²Œ ìƒì„± & ì €ì¥ ======\n",
        "prompt = [\"[BOS]\",\"COMPOSER_Mozart\",\"PERIOD_Middle\",\"GENRE_Sonata\",\"KEY_Cmin\",\n",
        "          \"TSig_4_4\",\"TEMPO_112\",\"BAR\",\"POS_0\",\"BAR\",\"POS_0\",\"BAR\",\"POS_0\"]\n",
        "\n",
        "tokens_long = stitch_generate(\n",
        "    prompt_tokens=prompt,\n",
        "    total_new=512,     # ê¸¸ì´ëŠ” ìœ ì§€\n",
        "    chunk_new=700,\n",
        "    context=480,\n",
        "    top_k=50, top_p=0.95, temp=0.9,\n",
        "    stop_on_eos=False\n",
        ")\n",
        "\n",
        "out_mid_long = SPLIT_ROOT / \"sample_beethoven_long_v1.mid\"\n",
        "detokenize_to_midi(tokens_long, out_mid_long)\n",
        "print(\"Saved â†’\", out_mid_long)\n",
        "\n",
        "# ====== 8) (ì„ íƒ) í¼í”Œë ‰ì„œí‹°ë¡œ ìƒíƒœ í™•ì¸ ======\n",
        "print(\"Best val loss:\", best_val, \"| approx PPL:\", math.exp(best_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejD7vA-HgOS0",
        "outputId": "9fa09cdb-9481-4651-fcd7-99f4616bfcb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/50] train 3.1018 | val 2.9482 | best 2.9482 | lr 0.000003\n",
            "[2/50] train 3.0835 | val 2.9442 | best 2.9442 | lr 0.000006\n",
            "[3/50] train 3.0633 | val 2.9568 | best 2.9442 | lr 0.000009\n",
            "[4/50] train 3.0389 | val 2.9537 | best 2.9442 | lr 0.000012\n",
            "[5/50] train 3.0170 | val 2.9465 | best 2.9442 | lr 0.000015\n",
            "[6/50] train 3.0054 | val 2.9490 | best 2.9442 | lr 0.000018\n",
            "[7/50] train 2.9993 | val 2.9420 | best 2.9420 | lr 0.000021\n",
            "[8/50] train 2.9986 | val 2.9555 | best 2.9420 | lr 0.000024\n",
            "[9/50] train 2.9865 | val 2.9406 | best 2.9406 | lr 0.000027\n",
            "[10/50] train 2.9873 | val 2.9527 | best 2.9406 | lr 0.000030\n",
            "[11/50] train 2.9921 | val 2.9548 | best 2.9406 | lr 0.000033\n",
            "[12/50] train 2.9947 | val 2.9597 | best 2.9406 | lr 0.000036\n",
            "[13/50] train 2.9844 | val 2.9508 | best 2.9406 | lr 0.000039\n",
            "[14/50] train 2.9849 | val 2.9557 | best 2.9406 | lr 0.000042\n",
            "[15/50] train 2.9740 | val 2.9500 | best 2.9406 | lr 0.000045\n",
            "[16/50] train 2.9781 | val 2.9574 | best 2.9406 | lr 0.000048\n",
            "[17/50] train 2.9642 | val 2.9438 | best 2.9406 | lr 0.000051\n",
            "[18/50] train 2.9673 | val 2.9573 | best 2.9406 | lr 0.000054\n",
            "[19/50] train 2.9630 | val 2.9538 | best 2.9406 | lr 0.000057\n",
            "[20/50] train 2.9637 | val 2.9532 | best 2.9406 | lr 0.000060\n",
            "[21/50] train 2.9417 | val 2.9467 | best 2.9406 | lr 0.000063\n",
            "[22/50] train 2.9350 | val 2.9649 | best 2.9406 | lr 0.000066\n",
            "[23/50] train 2.9306 | val 2.9702 | best 2.9406 | lr 0.000069\n",
            "[24/50] train 2.9035 | val 2.9514 | best 2.9406 | lr 0.000072\n",
            "[25/50] train 2.8737 | val 2.9400 | best 2.9400 | lr 0.000075\n",
            "[26/50] train 2.8484 | val 2.9554 | best 2.9400 | lr 0.000078\n",
            "[27/50] train 2.8161 | val 2.9479 | best 2.9400 | lr 0.000081\n",
            "[28/50] train 2.7889 | val 2.9478 | best 2.9400 | lr 0.000084\n",
            "[29/50] train 2.7708 | val 2.9256 | best 2.9256 | lr 0.000087\n",
            "[30/50] train 2.7533 | val 2.9354 | best 2.9256 | lr 0.000090\n",
            "[31/50] train 2.7335 | val 2.9170 | best 2.9170 | lr 0.000093\n",
            "[32/50] train 2.7352 | val 2.9556 | best 2.9170 | lr 0.000096\n",
            "[33/50] train 2.7125 | val 2.9615 | best 2.9170 | lr 0.000099\n",
            "[34/50] train 2.7062 | val 2.9294 | best 2.9170 | lr 0.000102\n",
            "[35/50] train 2.6925 | val 2.9423 | best 2.9170 | lr 0.000105\n",
            "[36/50] train 2.6830 | val 2.9428 | best 2.9170 | lr 0.000108\n",
            "[37/50] train 2.6728 | val 2.9594 | best 2.9170 | lr 0.000111\n",
            "[38/50] train 2.6552 | val 2.9329 | best 2.9170 | lr 0.000114\n",
            "[39/50] train 2.6571 | val 2.9399 | best 2.9170 | lr 0.000117\n",
            "[40/50] train 2.6465 | val 2.9272 | best 2.9170 | lr 0.000120\n",
            "[41/50] train 2.6568 | val 2.9208 | best 2.9170 | lr 0.000123\n",
            "[42/50] train 2.6414 | val 2.9323 | best 2.9170 | lr 0.000126\n",
            "[43/50] train 2.6422 | val 2.9331 | best 2.9170 | lr 0.000129\n",
            "[44/50] train 2.6232 | val 2.9393 | best 2.9170 | lr 0.000132\n",
            "[45/50] train 2.6168 | val 2.9353 | best 2.9170 | lr 0.000135\n",
            "[46/50] train 2.6130 | val 2.9405 | best 2.9170 | lr 0.000138\n",
            "[47/50] train 2.6143 | val 2.9390 | best 2.9170 | lr 0.000141\n",
            "[48/50] train 2.6087 | val 2.9201 | best 2.9170 | lr 0.000144\n",
            "[49/50] train 2.5892 | val 2.9343 | best 2.9170 | lr 0.000147\n",
            "[50/50] train 2.5953 | val 2.9172 | best 2.9170 | lr 0.000150\n",
            "[SWA] val 2.6301\n",
            "SWA checkpoint saved (better than EMA-best).\n",
            "Saved â†’ /content/drive/MyDrive/Deep_Learning_project/mozart_dataset_TF/sample_mozart_long_v2.mid\n",
            "Best val loss (EMA): 2.917028018406459 | approx PPL: 18.486264898215694\n"
          ]
        }
      ],
      "source": [
        "# ====== 0) ì¤€ë¹„ ======\n",
        "import math, torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.amp import GradScaler, autocast  # AMP ìµœì‹  API\n",
        "from torch.optim.swa_utils import AveragedModel  # SWA\n",
        "\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "PAD_ID = VOCAB[\"[PAD]\"]\n",
        "UNK_ID = VOCAB.get(\"[UNK]\", None)\n",
        "assert UNK_ID is not None, \"[UNK] í† í°ì´ vocabì— í•„ìš”í•©ë‹ˆë‹¤.\"\n",
        "\n",
        "# ----- (ì„ íƒ) í† í° ë“œë¡­ì•„ì›ƒ: VEL_/DUR_ ì†ŒëŸ‰ ë§ˆìŠ¤í‚¹ -----\n",
        "DROP_PROB = 0.05  # 3~7% ê¶Œì¥\n",
        "VEL_IDS = {tid for tok, tid in VOCAB.items() if tok.startswith(\"VEL_\")}\n",
        "DUR_IDS = {tid for tok, tid in VOCAB.items() if tok.startswith(\"DUR_\")}\n",
        "DROP_SET = VEL_IDS | DUR_IDS\n",
        "\n",
        "def token_dropout(batch_ids, drop_prob=DROP_PROB):\n",
        "    # batch_ids: LongTensor [B, T]\n",
        "    if drop_prob <= 0 or not DROP_SET:\n",
        "        return batch_ids\n",
        "    dev = batch_ids.device\n",
        "    drop_ids = torch.tensor(list(DROP_SET), device=dev)\n",
        "    mask = torch.rand_like(batch_ids.float()) < drop_prob\n",
        "    sel = mask & torch.isin(batch_ids, drop_ids)\n",
        "    return batch_ids.masked_fill(sel, PAD_ID)\n",
        "\n",
        "# ====== EMA ìœ í‹¸ ======\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.decay = decay\n",
        "        self.shadow = {k: p.detach().clone() for k, p in model.state_dict().items()}\n",
        "        self.backup = None\n",
        "    @torch.no_grad()\n",
        "    def update(self, model):\n",
        "        for k, p in model.state_dict().items():\n",
        "            self.shadow[k].mul_(self.decay).add_(p.detach(), alpha=1 - self.decay)\n",
        "    def store(self, model):\n",
        "        self.backup = {k: p.detach().clone() for k, p in model.state_dict().items()}\n",
        "    def copy_to(self, model):\n",
        "        model.load_state_dict(self.shadow, strict=False)\n",
        "    def restore(self, model):\n",
        "        if self.backup is not None:\n",
        "            model.load_state_dict(self.backup, strict=False)\n",
        "            self.backup = None\n",
        "\n",
        "ema = EMA(model, decay=0.999)\n",
        "\n",
        "# ====== 1) ì†ì‹¤, ì˜µí‹°ë§ˆì´ì €, ìŠ¤ì¼€ì¤„ëŸ¬ ======\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    ignore_index=PAD_ID,\n",
        "    label_smoothing=0.05,\n",
        ")\n",
        "\n",
        "opt = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=3e-4,\n",
        "    betas=(0.9, 0.95),\n",
        "    weight_decay=0.03,\n",
        ")\n",
        "\n",
        "EPOCHS = 50\n",
        "warmup_steps = 1000\n",
        "total_steps  = max(1, len(train_dl) * EPOCHS)\n",
        "\n",
        "def lr_lambda(step):\n",
        "    if step < warmup_steps:\n",
        "        return step / max(1, warmup_steps)\n",
        "    progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "    progress = min(1.0, max(0.0, progress))\n",
        "    return 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "sched = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
        "\n",
        "use_cuda_amp = (DEVICE.type == 'cuda')\n",
        "scaler = GradScaler('cuda' if use_cuda_amp else 'cpu')\n",
        "\n",
        "# ====== R-Drop ì„¤ì • ======\n",
        "kl_factor = 1.0  # 0.5~2.0 ì‚¬ì´ íƒìƒ‰ ì¶”ì²œ\n",
        "\n",
        "def sym_kl(logits1, logits2, mask=None):\n",
        "    # logits: [B, T, V]; mask: [B, T] (1=valid, 0=ignore)\n",
        "    p = F.log_softmax(logits1, dim=-1)\n",
        "    q = F.log_softmax(logits2, dim=-1)\n",
        "    p_exp = p.exp(); q_exp = q.exp()\n",
        "    kl = (p_exp * (p - q)).sum(-1) + (q_exp * (q - p)).sum(-1)  # [B, T]\n",
        "    if mask is not None:\n",
        "        kl = kl * mask\n",
        "        denom = mask.sum().clamp_min(1)\n",
        "        return kl.sum() / denom\n",
        "    return kl.mean()\n",
        "\n",
        "# ====== 2) í•™ìŠµ/ê²€ì¦ í•¨ìˆ˜ (ACC=2 + R-Drop + TokenDropout) ======\n",
        "ACC = 2  # ìœ íš¨ ë°°ì¹˜ x2\n",
        "\n",
        "def run_epoch(dl, train=True, grad_clip=1.0):\n",
        "    model.train(train)\n",
        "    total, n = 0.0, 0\n",
        "    if train:\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "    for step, (x, y) in enumerate(dl, 1):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        if train:\n",
        "            x = token_dropout(x)  # ì…ë ¥ ë…¸ì´ì¦ˆ(ì†ŒëŸ‰)ë¡œ ê°•ê±´ì„± â†‘\n",
        "\n",
        "        with torch.set_grad_enabled(train):\n",
        "            if train:\n",
        "                with autocast('cuda' if use_cuda_amp else 'cpu'):\n",
        "                    # R-Drop: dropout í™œì„± ìƒíƒœì—ì„œ ë‘ ë²ˆ forward\n",
        "                    logits1 = model(x)\n",
        "                    logits2 = model(x)\n",
        "                    ce1 = criterion(logits1.view(-1, VOCAB_SIZE), y.view(-1))\n",
        "                    ce2 = criterion(logits2.view(-1, VOCAB_SIZE), y.view(-1))\n",
        "                    y_mask = (y != PAD_ID).float()\n",
        "                    kl = sym_kl(logits1, logits2, y_mask)\n",
        "                    loss = 0.5*(ce1+ce2) + kl_factor*kl\n",
        "                    if ACC > 1:\n",
        "                        loss = loss / ACC\n",
        "            else:\n",
        "                with autocast('cuda' if use_cuda_amp else 'cpu'):\n",
        "                    logits = model(x)\n",
        "                    loss = criterion(logits.view(-1, VOCAB_SIZE), y.view(-1))\n",
        "\n",
        "        if train:\n",
        "            scaler.scale(loss).backward()\n",
        "            if step % ACC == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "                scaler.step(opt)\n",
        "                scaler.update()\n",
        "                sched.step()\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                ema.update(model)\n",
        "\n",
        "        total += loss.item() * (ACC if train and ACC > 1 else 1.0)\n",
        "        n += 1\n",
        "    return total / max(1, n)\n",
        "\n",
        "# ====== 3) SWA(ë§‰íŒ í‰ê· ) + EMA ê²€ì¦ + ë² ìŠ¤íŠ¸ ì €ì¥ ======\n",
        "best_val = float('inf')\n",
        "ckpt_path = SPLIT_ROOT / \"minigpt_best.pt\"\n",
        "\n",
        "swa_start_epoch = max(5, EPOCHS - 5)  # ë§ˆì§€ë§‰ 5ì—í­ í‰ê· \n",
        "swa_model = AveragedModel(model)\n",
        "\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr = run_epoch(train_dl, train=True)\n",
        "\n",
        "    # SWA í‰ê·  ëˆ„ì (ì—í­ ë‹¨ìœ„)\n",
        "    if ep >= swa_start_epoch:\n",
        "        swa_model.update_parameters(model)\n",
        "\n",
        "    # EMA ê°€ì¤‘ì¹˜ë¡œ ê²€ì¦\n",
        "    ema.store(model); ema.copy_to(model)\n",
        "    vl = run_epoch(val_dl, train=False)\n",
        "    ema.restore(model)\n",
        "\n",
        "    if vl < best_val:\n",
        "        best_val = vl\n",
        "        torch.save(model.state_dict(), str(ckpt_path))\n",
        "    print(f\"[{ep}/{EPOCHS}] train {tr:.4f} | val {vl:.4f} | best {best_val:.4f} | lr {sched.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "# ====== 4) ë² ìŠ¤íŠ¸ ê°€ì¤‘ì¹˜ ë¡œë“œ(ì•ˆì „) + (ì„ íƒ) SWA í‰ê°€/ì €ì¥ ======\n",
        "_ = model.load_state_dict(torch.load(str(ckpt_path), map_location=DEVICE))\n",
        "\n",
        "# SWA ê°€ì¤‘ì¹˜ë¡œë„ í•œ ë²ˆ í‰ê°€í•´ë³´ê³ , ë” ì¢‹ìœ¼ë©´ SWAë¡œ ì €ì¥\n",
        "try:\n",
        "    ema.store(model)\n",
        "    model.load_state_dict(swa_model.state_dict(), strict=False)\n",
        "    vl_swa = run_epoch(val_dl, train=False)\n",
        "    print(f\"[SWA] val {vl_swa:.4f}\")\n",
        "    if vl_swa < best_val:\n",
        "        torch.save(model.state_dict(), str(SPLIT_ROOT / \"minigpt_best_swa.pt\"))\n",
        "        print(\"SWA checkpoint saved (better than EMA-best).\")\n",
        "    ema.restore(model)\n",
        "except Exception as e:\n",
        "    print(\"SWA eval skipped:\", e)\n",
        "\n",
        "# ====== 5) ê¸°ë³¸ generate (ìœˆë„ìš° ìë™ crop + ê²½ëŸ‰ ë°˜ë³µ í˜ë„í‹°/ì˜¨ë„ ìŠ¤ì¼€ì¤„) ======\n",
        "def temp_schedule(step, t_max, t0=0.9, t1=0.8):\n",
        "    a = min(1.0, max(0.0, step / max(1, t_max)))\n",
        "    return t0*(1-a) + t1*a\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(prompt_tokens, max_new=700, top_k=50, top_p=0.95, temp=0.9, rep_penalty=1.05):\n",
        "    model.eval()\n",
        "    pos_len = getattr(getattr(model, 'pos', None), 'shape', [1, 100000, 0])[1]\n",
        "    ids = torch.tensor([[VOCAB.get(t, UNK_ID) for t in prompt_tokens]], device=DEVICE)\n",
        "    for step in range(max_new):\n",
        "        ids_win = ids[:, -pos_len:] if ids.size(1) > pos_len else ids\n",
        "        cur_temp = temp_schedule(step, max_new, t0=temp, t1=max(0.6, temp-0.1))\n",
        "        logits = model(ids_win)[:, -1, :] / max(cur_temp, 1e-6)\n",
        "        probs = torch.softmax(logits, dim=-1)[0]\n",
        "\n",
        "        # ë¯¸ì„¸ ë°˜ë³µ í˜ë„í‹°(ì§ì „ í† í°ë§Œ ì•½í•˜ê²Œ)\n",
        "        last_tok = ids[0, -1]\n",
        "        probs[last_tok] = probs[last_tok] / rep_penalty\n",
        "\n",
        "        if top_k > 0:\n",
        "            topk = torch.topk(probs, top_k)\n",
        "            mask = torch.ones_like(probs, dtype=torch.bool); mask[topk.indices] = False\n",
        "            probs = probs.masked_fill(mask, 0)\n",
        "        if top_p < 1.0:\n",
        "            sprob, sidx = torch.sort(probs, descending=True)\n",
        "            keep = torch.cumsum(sprob, dim=-1) <= top_p\n",
        "            keep[0] = True\n",
        "            mask = torch.ones_like(probs, dtype=torch.bool); mask[sidx[keep]] = False\n",
        "            probs = probs.masked_fill(mask, 0)\n",
        "        probs = probs / probs.sum()\n",
        "\n",
        "        nxt = torch.multinomial(probs, 1)\n",
        "        ids = torch.cat([ids, nxt.view(1,1)], dim=1)\n",
        "        if nxt.item() == VOCAB[\"[EOS]\"]:\n",
        "            break\n",
        "    return [IVOCAB[i.item()] for i in ids[0]]\n",
        "\n",
        "# ====== 6) ê¸¸ê²Œ ìƒì„±: ì²­í¬ ìŠ¤í‹°ì¹­(ëª¨ë¸ ìˆ˜ì • ì—†ìŒ) ======\n",
        "def stitch_generate(prompt_tokens, total_new=512, chunk_new=700, context=480,\n",
        "                    top_k=50, top_p=0.95, temp=0.9, stop_on_eos=False):\n",
        "    all_tokens = list(prompt_tokens); made = 0\n",
        "    while made < total_new:\n",
        "        this_new = min(chunk_new, total_new - made)\n",
        "        cur_prompt = all_tokens[-context:] if len(all_tokens) > context else all_tokens\n",
        "        chunk = generate(cur_prompt, max_new=this_new, top_k=top_k, top_p=top_p, temp=temp)\n",
        "        new_part = chunk[len(cur_prompt):] if len(chunk) > len(cur_prompt) else []\n",
        "        if stop_on_eos and (\"[EOS]\" in new_part):\n",
        "            eos_idx = new_part.index(\"[EOS]\"); all_tokens += new_part[:eos_idx]; break\n",
        "        all_tokens += new_part; made += len(new_part)\n",
        "        if len(new_part) == 0: break\n",
        "    return all_tokens\n",
        "\n",
        "# ====== 7) í”„ë¡¬í”„íŠ¸ ì„¤ì • & ê¸¸ê²Œ ìƒì„± & ì €ì¥ ======\n",
        "prompt = [\"[BOS]\",\"COMPOSER_Mozart\",\"PERIOD_Middle\",\"GENRE_Sonata\",\"KEY_Cmin\",\n",
        "          \"TSig_4_4\",\"TEMPO_112\",\"BAR\",\"POS_0\",\"BAR\",\"POS_0\",\"BAR\",\"POS_0\"]\n",
        "\n",
        "tokens_long = stitch_generate(\n",
        "    prompt_tokens=prompt,\n",
        "    total_new=512,   # ê¸¸ì´ëŠ” ìœ ì§€\n",
        "    chunk_new=700,\n",
        "    context=480,\n",
        "    top_k=50, top_p=0.95, temp=0.9,\n",
        "    stop_on_eos=False\n",
        ")\n",
        "\n",
        "out_mid_long = SPLIT_ROOT / \"sample_mozart_long_v2.mid\"\n",
        "detokenize_to_midi(tokens_long, out_mid_long)\n",
        "print(\"Saved â†’\", out_mid_long)\n",
        "\n",
        "# ====== 8) (ì„ íƒ) í¼í”Œë ‰ì„œí‹°ë¡œ ìƒíƒœ í™•ì¸ ======\n",
        "print(\"Best val loss (EMA):\", best_val, \"| approx PPL:\", math.exp(best_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pJYE9E3T3rL"
      },
      "outputs": [],
      "source": [
        "!pip install pyFluidSynth==1.3.4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVm3SK1wkHaa"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nApGl8Ek05d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Rx7y1bEBWUSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aaMtQ6ylXAiJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}