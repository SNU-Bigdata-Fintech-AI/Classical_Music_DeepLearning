{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngQefdQBMh-6",
        "outputId": "184389e4-6fd5-4d87-a4c4-08c9f9809c8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "총 MIDI: 32522개\n",
            "beethoven 후보: 450개\n",
            "메타 없음으로 스킵: 0개, 파일명 규칙 불일치 스킵: 0개\n",
            "\n",
            "=== 분할 결과 ===\n",
            "train: 270, validation: 90, test: 90\n",
            "저장(JSON): /content/drive/MyDrive/DL/beethoven_dataset/flattened_metadata_with_split.json\n",
            "저장(CSV):  /content/drive/MyDrive/DL/beethoven_dataset/flattened_metadata_with_split.csv\n",
            "출력 폴더: /content/drive/MyDrive/DL/beethoven_dataset (train/ validation/ test)\n",
            "\n",
            "(참고) optional 필드 결측 개수 → {'music_period': 30, 'difficulty': 215, 'genre': 2, 'opus': 25}\n"
          ]
        }
      ],
      "source": [
        "# === beethoven만 6:2:2로 분리 + flattened_metadata_with_split.json 생성 ===\n",
        "\n",
        "from pathlib import Path\n",
        "import os, json, re, math, shutil, glob, random\n",
        "\n",
        "# --- 경로 설정 ---\n",
        "PROJ = Path(\"/content/drive/MyDrive/DL/aria-midi-v1-unique-ext\").resolve()\n",
        "DATA_ROOT = PROJ / \"data\"\n",
        "META_JSON = PROJ / \"metadata.json\"\n",
        "\n",
        "SPLIT_ROOT = Path(\"/content/drive/MyDrive/DL/beethoven_dataset\")\n",
        "SPLIT_ROOT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 여러 번 실행 시, 아래를 True로 두면 train/validation/test 폴더를 비우고 시작합니다.\n",
        "CLEAN_SPLIT = True\n",
        "if CLEAN_SPLIT:\n",
        "    for sub in [\"train\", \"validation\", \"test\"]:\n",
        "        shutil.rmtree(SPLIT_ROOT / sub, ignore_errors=True)\n",
        "\n",
        "for sub in [\"train\", \"validation\", \"test\"]:\n",
        "    (SPLIT_ROOT / sub).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- 유틸 ---\n",
        "def parse_filename(fp: Path):\n",
        "    m = re.match(r\"^(\\d{6})_(\\d+)\\.mid$\", fp.name)\n",
        "    if not m:\n",
        "        return None, None\n",
        "    id_num = int(m.group(1))\n",
        "    take = m.group(2)\n",
        "    return str(id_num), take  # metadata.json의 키는 '4' 같은 형태\n",
        "\n",
        "def pick_audio_score(meta_entry: dict, take: str):\n",
        "    aud = meta_entry.get(\"audio_scores\", {})\n",
        "    if isinstance(aud, dict) and aud:\n",
        "        if take in aud:\n",
        "            return aud[take]\n",
        "        try:\n",
        "            return next(iter(aud.values()))\n",
        "        except StopIteration:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "def is_beethoven(composer_val):\n",
        "    if composer_val is None:\n",
        "        return False\n",
        "    return \"beethoven\" in str(composer_val).lower()\n",
        "\n",
        "# --- 메타 로드 ---\n",
        "assert META_JSON.exists(), f\"metadata.json not found: {META_JSON}\"\n",
        "with open(META_JSON, \"r\") as f:\n",
        "    meta_raw = json.load(f)\n",
        "\n",
        "# --- 데이터 스캔: 모든 .mid 파일 ---\n",
        "all_mid_paths = [Path(p) for p in glob.glob(str(DATA_ROOT / \"**\" / \"*.mid\"), recursive=True)]\n",
        "\n",
        "# --- Beethoven만 필터링 ---\n",
        "beet_items = []\n",
        "skipped_no_meta = 0\n",
        "skipped_bad_name = 0\n",
        "\n",
        "for fp in all_mid_paths:\n",
        "    id_str, take = parse_filename(fp)\n",
        "    if not id_str:\n",
        "        skipped_bad_name += 1\n",
        "        continue\n",
        "    entry = meta_raw.get(id_str)\n",
        "    if not entry:\n",
        "        skipped_no_meta += 1\n",
        "        continue\n",
        "\n",
        "    md = entry.get(\"metadata\", {})\n",
        "    if not is_beethoven(md.get(\"composer\")):\n",
        "        continue\n",
        "\n",
        "    beet_items.append((fp, id_str, take, entry))\n",
        "\n",
        "print(f\"총 MIDI: {len(all_mid_paths)}개\")\n",
        "print(f\"beethoven 후보: {len(beet_items)}개\")\n",
        "print(f\"메타 없음으로 스킵: {skipped_no_meta}개, 파일명 규칙 불일치 스킵: {skipped_bad_name}개\")\n",
        "\n",
        "# --- 6:2:2 분할 ---\n",
        "SEED = 42\n",
        "random.Random(SEED).shuffle(beet_items)\n",
        "\n",
        "N = len(beet_items)\n",
        "n_train = math.floor(N * 0.6)\n",
        "n_val   = math.floor(N * 0.2)\n",
        "n_test  = N - n_train - n_val\n",
        "\n",
        "splits = (\n",
        "    [(\"train\", 0.6)] * n_train +\n",
        "    [(\"validation\", 0.2)] * n_val +\n",
        "    [(\"test\", 0.2)] * n_test\n",
        ")\n",
        "\n",
        "# --- 복사 & 플래튼 메타 구성 ---\n",
        "flat_meta = {}  # key: 파일명, val: 메타 dict\n",
        "missing_optionals = {\"music_period\": 0, \"difficulty\": 0, \"genre\": 0, \"opus\": 0}\n",
        "\n",
        "for (item, (split_name, split_ratio)) in zip(beet_items, splits):\n",
        "    fp, id_str, take, entry = item\n",
        "    md = entry.get(\"metadata\", {})\n",
        "\n",
        "    basename = fp.name\n",
        "    audio_score = pick_audio_score(entry, take)\n",
        "\n",
        "    music_period = md.get(\"music_period\")\n",
        "    difficulty   = md.get(\"difficulty\")\n",
        "    genre        = md.get(\"genre\")\n",
        "    opus         = md.get(\"opus\")\n",
        "\n",
        "    if music_period is None: missing_optionals[\"music_period\"] += 1\n",
        "    if difficulty   is None: missing_optionals[\"difficulty\"]   += 1\n",
        "    if genre        is None: missing_optionals[\"genre\"]        += 1\n",
        "    if opus         is None: missing_optionals[\"opus\"]         += 1\n",
        "\n",
        "    dst = SPLIT_ROOT / split_name / basename\n",
        "    shutil.copy2(fp, dst)  # 같은 이름이면 덮어씀\n",
        "\n",
        "    flat_meta[basename] = {\n",
        "        \"file_path\": basename,       # ex) 000004_0.mid\n",
        "        \"split\": split_name,         # train / validation / test\n",
        "        \"composer\": md.get(\"composer\"),\n",
        "        \"music_period\": music_period,\n",
        "        \"difficulty\": difficulty,\n",
        "        \"genre\": genre,\n",
        "        \"audio_score\": audio_score,\n",
        "        \"opus\": opus,\n",
        "        \"split_ratio\": split_ratio,\n",
        "    }\n",
        "\n",
        "# --- JSON/CSV 저장: 쓰기 가능한 SPLIT_ROOT에 저장 ---\n",
        "OUT_JSON = SPLIT_ROOT / \"flattened_metadata_with_split.json\"\n",
        "OUT_CSV  = SPLIT_ROOT / \"flattened_metadata_with_split.csv\"\n",
        "\n",
        "with open(OUT_JSON, \"w\") as f:\n",
        "    json.dump(flat_meta, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# CSV도 같이 저장(편의)\n",
        "import pandas as pd\n",
        "pd.DataFrame.from_dict(flat_meta, orient=\"index\").reset_index(drop=True).to_csv(OUT_CSV, index=False)\n",
        "\n",
        "print(\"\\n=== 분할 결과 ===\")\n",
        "print(f\"train: {n_train}, validation: {n_val}, test: {n_test}\")\n",
        "print(f\"저장(JSON): {OUT_JSON}\")\n",
        "print(f\"저장(CSV):  {OUT_CSV}\")\n",
        "print(f\"출력 폴더: {SPLIT_ROOT} (train/ validation/ test)\")\n",
        "print(\"\\n(참고) optional 필드 결측 개수 →\", {k:v for k,v in missing_optionals.items() if v>0})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD9KN_NNQJF7",
        "outputId": "2d7fcbd8-96b4-47d6-a388-8dbbd9019e0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip -q install pretty_midi mido einops pyfluidsynth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "l-xZ2WxIR2FZ"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import json, random, math, os\n",
        "import numpy as np\n",
        "import pretty_midi as pm\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from einops import rearrange\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "SPLIT_ROOT = Path(\"/content/drive/MyDrive/DL/beethoven_dataset\")\n",
        "META_JSON  = SPLIT_ROOT / \"flattened_metadata_with_split.json\"\n",
        "assert META_JSON.exists(), META_JSON\n",
        "\n",
        "with open(META_JSON) as f:\n",
        "    META = json.load(f)\n",
        "\n",
        "# 토큰/캐시 경로\n",
        "VOCAB_JSON   = SPLIT_ROOT / \"vocab.json\"\n",
        "TOK_CACHE_DIR= SPLIT_ROOT / \"tok_cache\"\n",
        "TOK_CACHE_DIR.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "We7OVO4KXkSg"
      },
      "source": [
        "### 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LeTC46_fX0sU"
      },
      "outputs": [],
      "source": [
        "!pip -q install music21\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "P9qKqnZKX4FR"
      },
      "outputs": [],
      "source": [
        "from music21 import converter\n",
        "\n",
        "# 음이름 표기: MIDI pitch class → 'C','C#',...,'B'\n",
        "PC2NAME = ['C','C#','D','Eb','E','F','F#','G','Ab','A','Bb','B']\n",
        "\n",
        "def detect_key_with_music21(midi_path: Path):\n",
        "    \"\"\"music21로 전체 곡의 조성(장/단조)을 추정해 KEY 토큰을 돌려줍니다.\"\"\"\n",
        "    try:\n",
        "        s = converter.parse(str(midi_path))\n",
        "        k = s.analyze('key')\n",
        "        tonic_name = k.tonic.name  # e.g., 'C', 'E-'\n",
        "        # music21의 E- 같은 표기를 좀 더 일반적으로 변환\n",
        "        tonic_name = tonic_name.replace('-','b')  # E- → Eb\n",
        "        mode = 'maj' if k.mode.lower().startswith('maj') else 'min'\n",
        "        return f\"{tonic_name}{'maj' if mode=='maj' else 'min'}\"  # 예: Cmaj, Amin, Ebmaj\n",
        "    except Exception:\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xmX3QrI1X7GO"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# 기본 화음 템플릿(루트=0 기준의 pitch-class 집합)\n",
        "CHORD_TEMPLATES = {\n",
        "    'maj'       : {0,4,7},\n",
        "    'min'       : {0,3,7},\n",
        "    'dim'       : {0,3,6},\n",
        "    'aug'       : {0,4,8},\n",
        "    'dom7'      : {0,4,7,10},\n",
        "    'maj7'      : {0,4,7,11},\n",
        "    'min7'      : {0,3,7,10},\n",
        "    'halfdim7'  : {0,3,6,10},\n",
        "    'dim7'      : {0,3,6,9},\n",
        "}\n",
        "\n",
        "def best_chord_label(pitches):\n",
        "    \"\"\"\n",
        "    pitches: 리스트/셋 (MIDI pitch들) → 최적의 (root, quality) 반환. 없으면 None.\n",
        "    평가 기준: 템플릿 커버 비율 + 사이즈 근사성.\n",
        "    \"\"\"\n",
        "    if not pitches:\n",
        "        return None\n",
        "    pcs = sorted({p % 12 for p in pitches})\n",
        "    if not pcs:\n",
        "        return None\n",
        "\n",
        "    best = None\n",
        "    best_score = -1e9\n",
        "    for root in range(12):\n",
        "        shifted = {(pc - root) % 12 for pc in pcs}\n",
        "        for qual, templ in CHORD_TEMPLATES.items():\n",
        "            inter = len(shifted & templ)\n",
        "            # 템플릿 포함 비율 & 여분 패널티\n",
        "            cover = inter / max(1, len(templ))\n",
        "            extra_penalty = -0.15 * max(0, len(shifted - templ))\n",
        "            score = cover + extra_penalty\n",
        "            if score > best_score and inter >= 2:  # 최소 2음 이상 맞아야 화음으로 인정\n",
        "                best_score = score\n",
        "                best = (root, qual)\n",
        "    if best is None:\n",
        "        return None\n",
        "    root_name = PC2NAME[best[0]]\n",
        "    return f\"{root_name}:{best[1]}\"  # 예: C:maj, A:min, G:dom7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "eepKPSdCX_W3"
      },
      "outputs": [],
      "source": [
        "TS_DIV = 16\n",
        "MIN_VEL, MAX_VEL = 20, 100\n",
        "MIN_DUR, MAX_DUR = 1, 16\n",
        "\n",
        "def quantize_time(pm_obj, ts_div=TS_DIV):\n",
        "    ts = pm_obj.time_signature_changes or [pm.TimeSignature(4,4,0.0)]\n",
        "    tempo_times, tempi = pm_obj.get_tempo_changes()\n",
        "    tempo = float(tempi[0]) if len(tempi) else 120.0\n",
        "    num, den = ts[0].numerator, ts[0].denominator\n",
        "    beat_len = 60.0 / tempo\n",
        "    bar_sec = (4/den) * num * beat_len\n",
        "\n",
        "    notes=[]\n",
        "    for inst in pm_obj.instruments:\n",
        "        for n in inst.notes:\n",
        "            bar = int(n.start // bar_sec)\n",
        "            pos = int(((n.start - bar*bar_sec)/bar_sec)*ts_div); pos = max(0,min(ts_div-1,pos))\n",
        "            dur = int(((n.end - n.start)/bar_sec)*ts_div); dur = max(MIN_DUR, min(MAX_DUR, dur))\n",
        "            vel = int(np.clip(n.velocity, MIN_VEL, MAX_VEL))\n",
        "            notes.append((bar,pos,n.pitch,dur,vel))\n",
        "    notes.sort(key=lambda x:(x[0],x[1],x[2]))\n",
        "    return notes, (num,den), int(tempo), bar_sec\n",
        "\n",
        "def encode_remi_harmony(midi_path: Path, add_chords=True, chord_every='pos'):\n",
        "    \"\"\"\n",
        "    chord_every: 'bar' → 바 당 1개, 'beat' → 박자 단위(근사), 'pos' → POS 단위(onset 기준).\n",
        "    \"\"\"\n",
        "    pm_obj = pm.PrettyMIDI(str(midi_path))\n",
        "    notes,(num,den),tempo,bar_sec = quantize_time(pm_obj)\n",
        "\n",
        "    # 0) KEY(장/단조) 토큰\n",
        "    key_token = detect_key_with_music21(midi_path)  # 예: 'Cmaj', 'Amin', None\n",
        "\n",
        "    toks = []\n",
        "    toks.append(f\"TSig_{num}_{den}\")\n",
        "    toks.append(f\"TEMPO_{tempo}\")\n",
        "    if key_token:\n",
        "        toks.append(f\"KEY_{key_token}\")  # 예: KEY_Cmaj\n",
        "\n",
        "    # 1) 화음 라벨링을 위한 그룹핑\n",
        "    #    - chord_every == 'bar' : 같은 bar 내 모든 노트 onsets\n",
        "    #    - chord_every == 'pos' : (bar,pos) 단위 노트 onsets\n",
        "    #    - chord_every == 'beat': bar 내 beat 경계 근사 (num 개)\n",
        "    from collections import defaultdict\n",
        "    onset_map = defaultdict(list)  # key: (bar, pos or beatIndex) → pitches\n",
        "\n",
        "    if chord_every == 'bar':\n",
        "        for (bar,pos,pitch,dur,vel) in notes:\n",
        "            onset_map[(bar, -1)].append(pitch)\n",
        "\n",
        "    elif chord_every == 'beat':\n",
        "        # 박자 경계(대략)로 pos→beat index 매핑 (TS_DIV를 num로 나눔)\n",
        "        step_per_beat = max(1, TS_DIV // num)\n",
        "        for (bar,pos,pitch,dur,vel) in notes:\n",
        "            beat_idx = pos // step_per_beat\n",
        "            onset_map[(bar, beat_idx)].append(pitch)\n",
        "\n",
        "    else:  # 'pos'\n",
        "        for (bar,pos,pitch,dur,vel) in notes:\n",
        "            onset_map[(bar, pos)].append(pitch)\n",
        "\n",
        "    # 2) 토큰 시퀀스 생성\n",
        "    cur_bar = -1\n",
        "    last_chord = None\n",
        "    for (bar,pos,pitch,dur,vel) in notes:\n",
        "        # BAR 토큰\n",
        "        while cur_bar < bar:\n",
        "            toks.append(\"BAR\")\n",
        "            cur_bar += 1\n",
        "            last_chord = None  # 새 마디에서 화음 새로 판단\n",
        "\n",
        "        # POS 토큰\n",
        "        toks.append(f\"POS_{pos}\")\n",
        "\n",
        "        # 2-a) 화음 토큰(선택)\n",
        "        if add_chords:\n",
        "            key = (bar, -1) if chord_every=='bar' else ((bar, pos) if chord_every=='pos' else (bar, pos // max(1, TS_DIV // num)))\n",
        "            chord_label = best_chord_label(onset_map.get(key, []))\n",
        "            if chord_label and chord_label != last_chord:\n",
        "                toks.append(f\"CHORD_{chord_label}\")   # 예: CHORD_C:maj, CHORD_A:min\n",
        "                last_chord = chord_label\n",
        "\n",
        "        # 2-b) 음표 이벤트\n",
        "        toks += [f\"NOTE_ON_{pitch}\", f\"DUR_{dur}\", f\"VEL_{vel}\"]\n",
        "\n",
        "    return toks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "oUhK5vZCYD4N"
      },
      "outputs": [],
      "source": [
        "def cond_tokens(meta, midi_path_for_key: Path = None):\n",
        "    t = [\"COMPOSER_Beethoven\"]\n",
        "    if meta.get(\"music_period\"): t.append(f\"PERIOD_{meta['music_period']}\")\n",
        "    if meta.get(\"genre\"):        t.append(f\"GENRE_{meta['genre']}\")\n",
        "    if meta.get(\"difficulty\"):   t.append(f\"DIFF_{meta['difficulty']}\")\n",
        "    if meta.get(\"opus\"):         t.append(f\"OPUS_{str(meta['opus']).replace(' ','_')}\")\n",
        "    q=meta.get(\"audio_score\")\n",
        "    if q is not None:\n",
        "        t.append(f\"QUALITY_{'High' if q>=0.8 else 'Med' if q>=0.5 else 'Low'}\")\n",
        "    # KEY (메타/파일 기반)\n",
        "    if midi_path_for_key is not None:\n",
        "        k = detect_key_with_music21(midi_path_for_key)\n",
        "        if k:\n",
        "            t.append(f\"KEY_{k}\")  # KEY_Cmaj / KEY_Amin ...\n",
        "    return t\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_g9y46PUSQ4U",
        "outputId": "4a8c110e-9e41-4351-ace7-53894848360d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VOCAB_SIZE: 266\n"
          ]
        }
      ],
      "source": [
        "def build_vocab():\n",
        "    if VOCAB_JSON.exists(): return json.load(open(VOCAB_JSON))\n",
        "    vocab={\"[PAD]\":0,\"[BOS]\":1,\"[EOS]\":2,\"[UNK]\":3}\n",
        "    idx=len(vocab)\n",
        "    for fname,meta in META.items():\n",
        "        p = SPLIT_ROOT/meta[\"split\"]/meta[\"file_path\"]\n",
        "        if not p.exists(): continue\n",
        "        # vocab 빌드 단계\n",
        "        toks = [\"[BOS]\"] + cond_tokens(meta, midi_path_for_key=p) + encode_remi_harmony(p, add_chords=True, chord_every='pos')+ [\"[EOS]\"]\n",
        "\n",
        "        json.dump(toks, open(TOK_CACHE_DIR/(fname+\".json\"),\"w\"))\n",
        "        for t in toks:\n",
        "            if t not in vocab:\n",
        "                vocab[t]=idx; idx+=1\n",
        "    json.dump(vocab, open(VOCAB_JSON,\"w\"))\n",
        "    return vocab\n",
        "\n",
        "VOCAB = build_vocab()\n",
        "IVOCAB= {i:t for t,i in VOCAB.items()}\n",
        "VOCAB_SIZE=len(VOCAB)\n",
        "print(\"VOCAB_SIZE:\", VOCAB_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wNqiWgG6SUWq"
      },
      "outputs": [],
      "source": [
        "SEQ_LEN=512\n",
        "\n",
        "def toks_to_ids(toks): return [VOCAB.get(t, VOCAB[\"[UNK]\"]) for t in toks]\n",
        "\n",
        "class BeethovenMIDIDataset(Dataset):\n",
        "    def __init__(self, split):\n",
        "        self.items=[(k,v) for k,v in META.items() if v[\"split\"]==split]\n",
        "        random.Random(42).shuffle(self.items)\n",
        "    def __len__(self): return len(self.items)\n",
        "    def __getitem__(self,i):\n",
        "        fname,meta = self.items[i]\n",
        "        cache=TOK_CACHE_DIR/(fname+\".json\")\n",
        "        if cache.exists(): toks=json.load(open(cache))\n",
        "        else:\n",
        "            p=SPLIT_ROOT/meta[\"split\"]/meta[\"file_path\"]\n",
        "            toks=[\"[BOS]\"]+cond_tokens(meta)+encode_remi_lite(p)+[\"[EOS]\"]\n",
        "        ids=toks_to_ids(toks)\n",
        "        if len(ids)>=SEQ_LEN:\n",
        "            st=random.randint(0, len(ids)-SEQ_LEN)\n",
        "            seq=ids[st:st+SEQ_LEN]\n",
        "        else:\n",
        "            seq=ids+[VOCAB[\"[PAD]\"]] * (SEQ_LEN-len(ids))\n",
        "        x=torch.tensor(seq[:-1],dtype=torch.long)\n",
        "        y=torch.tensor(seq[1:], dtype=torch.long)\n",
        "        return x,y\n",
        "\n",
        "train_dl=DataLoader(BeethovenMIDIDataset(\"train\"), batch_size=16, shuffle=True, drop_last=True)\n",
        "val_dl  =DataLoader(BeethovenMIDIDataset(\"validation\"), batch_size=16, shuffle=False, drop_last=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "O_BC3kuQSf6q"
      },
      "outputs": [],
      "source": [
        "import math, torch, torch.nn as nn\n",
        "from einops import rearrange\n",
        "\n",
        "# ===== 1) RoPE (Rotary Positional Embedding) =====\n",
        "def apply_rope(q,k):\n",
        "    B,H,T,D = q.shape\n",
        "    pos = torch.arange(T, device=q.device).float()\n",
        "    inv = 1.0/(10000**(torch.arange(0,D,2,device=q.device).float()/D))\n",
        "    ang = torch.einsum('t,d->td', pos, inv)\n",
        "    sin,cos = ang.sin()[None,None], ang.cos()[None,None]\n",
        "    def rot(x):\n",
        "        x1,x2=x[...,::2],x[...,1::2]\n",
        "        return torch.stack([x1*cos - x2*sin, x1*sin + x2*cos], dim=-1).flatten(-2)\n",
        "    return rot(q), rot(k)\n",
        "\n",
        "# ===== 2) Self-Attention =====\n",
        "class CausalSelfAttn(nn.Module):\n",
        "    def __init__(self,d_model=512,n_head=8,p=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model% n_head==0\n",
        "        self.nh=n_head; self.dk=d_model//n_head\n",
        "        self.qkv=nn.Linear(d_model, d_model*3)\n",
        "        self.proj=nn.Linear(d_model, d_model)\n",
        "        self.drop=nn.Dropout(p)\n",
        "    def forward(self,x):\n",
        "        B,T,C=x.shape\n",
        "        q,k,v = self.qkv(x).chunk(3,-1)\n",
        "        q=rearrange(q,'b t (h d)->b h t d',h=self.nh)\n",
        "        k=rearrange(k,'b t (h d)->b h t d',h=self.nh)\n",
        "        v=rearrange(v,'b t (h d)->b h t d',h=self.nh)\n",
        "        q,k=apply_rope(q,k)\n",
        "        att=(q@k.transpose(-1,-2))/math.sqrt(self.dk)\n",
        "        mask=torch.triu(torch.ones(T,T,device=x.device),1).bool()\n",
        "        att=att.masked_fill(mask,float('-inf')).softmax(-1)\n",
        "        att=self.drop(att)\n",
        "        y=att@v\n",
        "        y=rearrange(y,'b h t d->b t (h d)')\n",
        "        return self.proj(y)\n",
        "\n",
        "# ===== 3) Transformer Block =====\n",
        "class Block(nn.Module):\n",
        "    def __init__(self,d=512,h=8,p=0.1,mlp=4):\n",
        "        super().__init__()\n",
        "        self.ln1=nn.LayerNorm(d); self.att=CausalSelfAttn(d,h,p)\n",
        "        self.ln2=nn.LayerNorm(d)\n",
        "        self.mlp=nn.Sequential(\n",
        "            nn.Linear(d,d*mlp), nn.GELU(), nn.Dropout(p), nn.Linear(d*mlp,d)\n",
        "        )\n",
        "        self.drop=nn.Dropout(p)\n",
        "    def forward(self,x):\n",
        "        x=x+self.drop(self.att(self.ln1(x)))\n",
        "        x=x+self.drop(self.mlp(self.ln2(x)))\n",
        "        return x\n",
        "\n",
        "# ===== 4) MiniGPT Model =====\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self,vocab_size, d=512, L=8, H=8, p=0.1):\n",
        "        super().__init__()\n",
        "        self.emb=nn.Embedding(vocab_size, d)\n",
        "        self.pos=nn.Parameter(torch.zeros(1, SEQ_LEN-1, d))  # 절대 위치 (RoPE와 병용)\n",
        "        self.blocks=nn.ModuleList([Block(d,H,p) for _ in range(L)])\n",
        "        self.ln=nn.LayerNorm(d)\n",
        "        self.head=nn.Linear(d, vocab_size, bias=False)\n",
        "    def forward(self,idx):\n",
        "        x=self.emb(idx) + self.pos[:, :idx.size(1), :]\n",
        "        for b in self.blocks: x=b(x)\n",
        "        return self.head(self.ln(x))\n",
        "\n",
        "# ===== 5) 모델 초기화 =====\n",
        "model = MiniGPT(VOCAB_SIZE).to(DEVICE)\n",
        "opt   = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9,0.95), weight_decay=0.1)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=VOCAB[\"[PAD]\"], label_smoothing=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "HWRsoGe7apIg"
      },
      "outputs": [],
      "source": [
        "import pretty_midi as pm\n",
        "from pathlib import Path\n",
        "\n",
        "def detokenize_to_midi(tokens, out_path, ts_div=16, default_time_sig=(4,4), default_tempo=120,\n",
        "                       program=0, safe_pitch=(36,96)):\n",
        "    \"\"\"\n",
        "    REMI류 토큰 시퀀스를 MIDI로 복원합니다.\n",
        "    - tokens 예시: [\"TSig_4_4\",\"TEMPO_112\",\"BAR\",\"POS_0\",\"NOTE_ON_60\",\"DUR_4\",\"VEL_80\", ...]\n",
        "    - KEY_*, CHORD_* 토큰은 MIDI에 직접 반영하지 않고 건너뜁니다.\n",
        "    - ts_div는 토큰화 때 쓴 TS_DIV와 동일해야 합니다(기본 16).\n",
        "    \"\"\"\n",
        "    out_path = Path(out_path) if isinstance(out_path, str) else out_path\n",
        "\n",
        "    # 초기 메타\n",
        "    num, den = default_time_sig\n",
        "    tempo = default_tempo\n",
        "    beat_len = 60.0 / tempo\n",
        "    bar_sec = (4/den) * num * beat_len\n",
        "\n",
        "    cur_bar = -1\n",
        "    pos = 0\n",
        "\n",
        "    pm_out = pm.PrettyMIDI()\n",
        "    inst = pm.Instrument(program=program)\n",
        "    SAFE_LOW, SAFE_HIGH = safe_pitch\n",
        "\n",
        "    i = 0\n",
        "    N = len(tokens)\n",
        "    while i < N:\n",
        "        t = tokens[i]\n",
        "\n",
        "        # 메타 토큰\n",
        "        if t.startswith(\"TSig_\"):\n",
        "            try:\n",
        "                _, a, b = t.split(\"_\")\n",
        "                num, den = int(a), int(b)\n",
        "                beat_len = 60.0 / tempo\n",
        "                bar_sec = (4/den) * num * beat_len\n",
        "            except Exception:\n",
        "                pass\n",
        "            i += 1; continue\n",
        "\n",
        "        if t.startswith(\"TEMPO_\"):\n",
        "            try:\n",
        "                tempo = int(t.split(\"_\")[1])\n",
        "                beat_len = 60.0 / tempo\n",
        "                bar_sec = (4/den) * num * beat_len\n",
        "            except Exception:\n",
        "                pass\n",
        "            i += 1; continue\n",
        "\n",
        "        if t.startswith(\"KEY_\") or t.startswith(\"CHORD_\"):\n",
        "            i += 1; continue  # 조성/화음 토큰은 MIDI에 직접 반영하지 않음\n",
        "\n",
        "        # 구조 토큰\n",
        "        if t == \"BAR\":\n",
        "            cur_bar += 1\n",
        "            pos = 0\n",
        "            i += 1; continue\n",
        "\n",
        "        if t.startswith(\"POS_\"):\n",
        "            try:\n",
        "                pos = int(t.split(\"_\")[1])\n",
        "                pos = max(0, min(ts_div-1, pos))\n",
        "            except Exception:\n",
        "                pos = max(0, min(ts_div-1, pos))\n",
        "            i += 1; continue\n",
        "\n",
        "        # 음표 이벤트\n",
        "        if t.startswith(\"NOTE_ON_\"):\n",
        "            # NOTE_ON_p\n",
        "            try:\n",
        "                pitch = int(t.split(\"_\")[2])\n",
        "            except Exception:\n",
        "                i += 1; continue\n",
        "            i += 1\n",
        "\n",
        "            # DUR_d (기본 4), VEL_v (기본 80)\n",
        "            dur = 4\n",
        "            vel = 80\n",
        "            if i < N and tokens[i].startswith(\"DUR_\"):\n",
        "                try: dur = int(tokens[i].split(\"_\")[1])\n",
        "                except Exception: pass\n",
        "                i += 1\n",
        "            if i < N and (tokens[i].startswith(\"VEL_\") or tokens[i].startswith(\"VELOCITY_\")):\n",
        "                try: vel = int(tokens[i].split(\"_\")[1])\n",
        "                except Exception: pass\n",
        "                i += 1\n",
        "\n",
        "            pitch = max(SAFE_LOW, min(SAFE_HIGH, pitch))\n",
        "            start = (cur_bar * bar_sec) + (pos / ts_div) * bar_sec\n",
        "            end   = start + (dur / ts_div) * bar_sec\n",
        "            if end <= start:\n",
        "                end = start + (1 / ts_div) * bar_sec  # 최소 1틱\n",
        "\n",
        "            inst.notes.append(pm.Note(velocity=vel, pitch=pitch, start=start, end=end))\n",
        "            continue\n",
        "\n",
        "        # 알 수 없는 토큰은 무시\n",
        "        i += 1\n",
        "\n",
        "    pm_out.instruments.append(inst)\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    pm_out.write(str(out_path))\n",
        "    return out_path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbeE1fjcZg71",
        "outputId": "fa941b97-36ea-4d2b-9c38-cc2adda5ffb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/30] train 2.5999 | val 2.5388 | best 2.5388 | lr 0.000002\n",
            "[2/30] train 2.5806 | val 2.5316 | best 2.5316 | lr 0.000005\n",
            "[3/30] train 2.5749 | val 2.5734 | best 2.5316 | lr 0.000007\n",
            "[4/30] train 2.5843 | val 2.5571 | best 2.5316 | lr 0.000010\n",
            "[5/30] train 2.5750 | val 2.5717 | best 2.5316 | lr 0.000012\n",
            "[6/30] train 2.5720 | val 2.5730 | best 2.5316 | lr 0.000014\n",
            "[7/30] train 2.5613 | val 2.5565 | best 2.5316 | lr 0.000017\n",
            "[8/30] train 2.5763 | val 2.5681 | best 2.5316 | lr 0.000019\n",
            "[9/30] train 2.5686 | val 2.5556 | best 2.5316 | lr 0.000022\n",
            "[10/30] train 2.5770 | val 2.5395 | best 2.5316 | lr 0.000024\n",
            "[11/30] train 2.5420 | val 2.5738 | best 2.5316 | lr 0.000026\n",
            "[12/30] train 2.5352 | val 2.5804 | best 2.5316 | lr 0.000029\n",
            "[13/30] train 2.5485 | val 2.5690 | best 2.5316 | lr 0.000031\n",
            "[14/30] train 2.5367 | val 2.5294 | best 2.5294 | lr 0.000034\n",
            "[15/30] train 2.5557 | val 2.5459 | best 2.5294 | lr 0.000036\n",
            "[16/30] train 2.5434 | val 2.5819 | best 2.5294 | lr 0.000038\n",
            "[17/30] train 2.5550 | val 2.5480 | best 2.5294 | lr 0.000041\n",
            "[18/30] train 2.5343 | val 2.5390 | best 2.5294 | lr 0.000043\n",
            "[19/30] train 2.5273 | val 2.5315 | best 2.5294 | lr 0.000046\n",
            "[20/30] train 2.5431 | val 2.5546 | best 2.5294 | lr 0.000048\n",
            "[21/30] train 2.5228 | val 2.5689 | best 2.5294 | lr 0.000050\n",
            "[22/30] train 2.5196 | val 2.5717 | best 2.5294 | lr 0.000053\n",
            "[23/30] train 2.5311 | val 2.5505 | best 2.5294 | lr 0.000055\n",
            "[24/30] train 2.5104 | val 2.5438 | best 2.5294 | lr 0.000058\n",
            "[25/30] train 2.5102 | val 2.5655 | best 2.5294 | lr 0.000060\n",
            "[26/30] train 2.5256 | val 2.5418 | best 2.5294 | lr 0.000062\n",
            "[27/30] train 2.5068 | val 2.5597 | best 2.5294 | lr 0.000065\n",
            "[28/30] train 2.5235 | val 2.5455 | best 2.5294 | lr 0.000067\n",
            "[29/30] train 2.5075 | val 2.5639 | best 2.5294 | lr 0.000070\n",
            "[30/30] train 2.5210 | val 2.5359 | best 2.5294 | lr 0.000072\n",
            "Saved → /content/drive/MyDrive/DL/beethoven_dataset/sample_beethoven_long_v1.mid\n",
            "Best val loss: 2.529429316520691 | approx PPL: 12.546344101930458\n"
          ]
        }
      ],
      "source": [
        "# ====== 0) 준비 ======\n",
        "import math, torch\n",
        "import torch.nn as nn\n",
        "from torch.amp import GradScaler, autocast  # ✅ AMP 최신 API\n",
        "\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "\n",
        "# ====== EMA 유틸 ======\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.decay = decay\n",
        "        self.shadow = {k: p.detach().clone() for k, p in model.state_dict().items()}\n",
        "        self.backup = None\n",
        "    @torch.no_grad()\n",
        "    def update(self, model):\n",
        "        for k, p in model.state_dict().items():\n",
        "            self.shadow[k].mul_(self.decay).add_(p.detach(), alpha=1 - self.decay)\n",
        "    def store(self, model):\n",
        "        self.backup = {k: p.detach().clone() for k, p in model.state_dict().items()}\n",
        "    def copy_to(self, model):\n",
        "        model.load_state_dict(self.shadow, strict=False)\n",
        "    def restore(self, model):\n",
        "        if self.backup is not None:\n",
        "            model.load_state_dict(self.backup, strict=False)\n",
        "            self.backup = None\n",
        "\n",
        "ema = EMA(model, decay=0.999)\n",
        "\n",
        "# ====== 1) 손실, 옵티마이저, 스케줄러, 체크포인트 ======\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    ignore_index=VOCAB[\"[PAD]\"],\n",
        "    label_smoothing=0.05,    # ✅ 0.05로 미세조정\n",
        ")\n",
        "\n",
        "opt = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=3e-4,\n",
        "    betas=(0.9, 0.95),\n",
        "    weight_decay=0.03,       # ✅ 과규제 완화\n",
        ")\n",
        "\n",
        "EPOCHS = 30\n",
        "warmup_steps = 1000\n",
        "total_steps  = max(1, len(train_dl) * EPOCHS)\n",
        "\n",
        "def lr_lambda(step):\n",
        "    if step < warmup_steps:\n",
        "        return step / max(1, warmup_steps)\n",
        "    progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "    progress = min(1.0, max(0.0, progress))\n",
        "    return 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "sched = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
        "\n",
        "use_cuda_amp = (DEVICE.type == 'cuda')\n",
        "scaler = GradScaler('cuda' if use_cuda_amp else 'cpu')\n",
        "\n",
        "# ====== 2) 학습/검증 함수 (ACC=2) ======\n",
        "ACC = 2  # ✅ 유효 배치 x2\n",
        "\n",
        "def run_epoch(dl, train=True, grad_clip=1.0):\n",
        "    model.train(train)\n",
        "    total, n = 0.0, 0\n",
        "    if train:\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "    for step, (x, y) in enumerate(dl, 1):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        with torch.set_grad_enabled(train):\n",
        "            with autocast('cuda' if use_cuda_amp else 'cpu'):\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits.view(-1, VOCAB_SIZE), y.view(-1))\n",
        "                if train and ACC > 1:\n",
        "                    loss = loss / ACC\n",
        "        if train:\n",
        "            scaler.scale(loss).backward()\n",
        "            if step % ACC == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "                scaler.step(opt)\n",
        "                scaler.update()\n",
        "                sched.step()\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                ema.update(model)  # ✅ step 뒤 EMA 업데이트\n",
        "        total += loss.item() * (ACC if train and ACC > 1 else 1.0)\n",
        "        n += 1\n",
        "    return total / max(1, n)\n",
        "\n",
        "# ====== 3) 학습 루프 + EMA 검증 + 베스트 저장 ======\n",
        "best_val = float('inf')\n",
        "ckpt_path = SPLIT_ROOT / \"minigpt_best.pt\"\n",
        "\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr = run_epoch(train_dl, train=True)\n",
        "\n",
        "    # ✅ EMA 가중치로 검증\n",
        "    ema.store(model); ema.copy_to(model)\n",
        "    vl = run_epoch(val_dl, train=False)\n",
        "    ema.restore(model)\n",
        "\n",
        "    if vl < best_val:\n",
        "        best_val = vl\n",
        "        torch.save(model.state_dict(), str(ckpt_path))\n",
        "    print(f\"[{ep}/{EPOCHS}] train {tr:.4f} | val {vl:.4f} | best {best_val:.4f} | lr {sched.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "# ====== 4) 베스트 가중치 로드(안전) ======\n",
        "_ = model.load_state_dict(torch.load(str(ckpt_path), map_location=DEVICE))\n",
        "\n",
        "# ====== 5) 기본 generate (윈도우 자동 적용: pos 임베딩 길이에 맞춰 crop) ======\n",
        "@torch.no_grad()\n",
        "def generate(prompt_tokens, max_new=700, top_k=50, top_p=0.95, temp=0.9):\n",
        "    model.eval()\n",
        "    unk_id = VOCAB.get(\"[UNK]\", None)\n",
        "    if unk_id is None:\n",
        "        raise ValueError(\"Vocab must contain [UNK] token.\")\n",
        "    pos_len = getattr(getattr(model, 'pos', None), 'shape', [1, 100000, 0])[1]\n",
        "    ids = torch.tensor([[VOCAB.get(t, unk_id) for t in prompt_tokens]], device=DEVICE)\n",
        "    for _ in range(max_new):\n",
        "        ids_win = ids[:, -pos_len:] if ids.size(1) > pos_len else ids\n",
        "        logits = model(ids_win)[:, -1, :] / max(temp, 1e-6)\n",
        "        probs = torch.softmax(logits, dim=-1)[0]\n",
        "        if top_k > 0:\n",
        "            topk = torch.topk(probs, top_k)\n",
        "            mask = torch.ones_like(probs, dtype=torch.bool); mask[topk.indices] = False\n",
        "            probs = probs.masked_fill(mask, 0)\n",
        "        if top_p < 1.0:\n",
        "            sprob, sidx = torch.sort(probs, descending=True)\n",
        "            keep = torch.cumsum(sprob, dim=-1) <= top_p\n",
        "            keep[0] = True\n",
        "            mask = torch.ones_like(probs, dtype=torch.bool); mask[sidx[keep]] = False\n",
        "            probs = probs.masked_fill(mask, 0)\n",
        "        probs = probs / probs.sum()\n",
        "        nxt = torch.multinomial(probs, 1)\n",
        "        ids = torch.cat([ids, nxt.view(1,1)], dim=1)\n",
        "        if nxt.item() == VOCAB[\"[EOS]\"]:\n",
        "            break\n",
        "    return [IVOCAB[i.item()] for i in ids[0]]\n",
        "\n",
        "# ====== 6) 길게 생성: 청크 스티칭(모델 수정 없음) ======\n",
        "def stitch_generate(prompt_tokens, total_new=512, chunk_new=700, context=480,\n",
        "                    top_k=50, top_p=0.95, temp=0.9, stop_on_eos=False):\n",
        "    all_tokens = list(prompt_tokens)\n",
        "    made = 0\n",
        "    while made < total_new:\n",
        "        this_new = min(chunk_new, total_new - made)\n",
        "        cur_prompt = all_tokens[-context:] if len(all_tokens) > context else all_tokens\n",
        "        chunk = generate(cur_prompt, max_new=this_new, top_k=top_k, top_p=top_p, temp=temp)\n",
        "        new_part = chunk[len(cur_prompt):] if len(chunk) > len(cur_prompt) else []\n",
        "        if stop_on_eos and (\"[EOS]\" in new_part):\n",
        "            eos_idx = new_part.index(\"[EOS]\"); all_tokens += new_part[:eos_idx]; break\n",
        "        all_tokens += new_part\n",
        "        made += len(new_part)\n",
        "        if len(new_part) == 0:\n",
        "            break\n",
        "    return all_tokens\n",
        "\n",
        "# ====== 7) 프롬프트 설정 & 길게 생성 & 저장 ======\n",
        "prompt = [\"[BOS]\",\"COMPOSER_Beethoven\",\"PERIOD_Middle\",\"GENRE_Sonata\",\"KEY_Cmin\",\n",
        "          \"TSig_4_4\",\"TEMPO_112\",\"BAR\",\"POS_0\",\"BAR\",\"POS_0\",\"BAR\",\"POS_0\"]\n",
        "\n",
        "tokens_long = stitch_generate(\n",
        "    prompt_tokens=prompt,\n",
        "    total_new=512,     # 길이는 유지\n",
        "    chunk_new=700,\n",
        "    context=480,\n",
        "    top_k=50, top_p=0.95, temp=0.9,\n",
        "    stop_on_eos=False\n",
        ")\n",
        "\n",
        "out_mid_long = SPLIT_ROOT / \"sample_beethoven_long_v1.mid\"\n",
        "detokenize_to_midi(tokens_long, out_mid_long)\n",
        "print(\"Saved →\", out_mid_long)\n",
        "\n",
        "# ====== 8) (선택) 퍼플렉서티로 상태 확인 ======\n",
        "print(\"Best val loss:\", best_val, \"| approx PPL:\", math.exp(best_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejD7vA-HgOS0",
        "outputId": "f7fc6886-cb90-4aa3-8ec1-e7b46d0e3158"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/30] train 2.7837 | val 2.5248 | best 2.5248 | lr 0.000002\n",
            "[2/30] train 2.7764 | val 2.5534 | best 2.5248 | lr 0.000005\n",
            "[3/30] train 2.7591 | val 2.5360 | best 2.5248 | lr 0.000007\n",
            "[4/30] train 2.7298 | val 2.5534 | best 2.5248 | lr 0.000010\n",
            "[5/30] train 2.7250 | val 2.5295 | best 2.5248 | lr 0.000012\n",
            "[6/30] train 2.7165 | val 2.5519 | best 2.5248 | lr 0.000014\n",
            "[7/30] train 2.7070 | val 2.5245 | best 2.5245 | lr 0.000017\n",
            "[8/30] train 2.6823 | val 2.5350 | best 2.5245 | lr 0.000019\n",
            "[9/30] train 2.6765 | val 2.5348 | best 2.5245 | lr 0.000022\n",
            "[10/30] train 2.6664 | val 2.5092 | best 2.5092 | lr 0.000024\n",
            "[11/30] train 2.6816 | val 2.5196 | best 2.5092 | lr 0.000026\n",
            "[12/30] train 2.6524 | val 2.5562 | best 2.5092 | lr 0.000029\n",
            "[13/30] train 2.6566 | val 2.5474 | best 2.5092 | lr 0.000031\n",
            "[14/30] train 2.6543 | val 2.5330 | best 2.5092 | lr 0.000034\n",
            "[15/30] train 2.6536 | val 2.5107 | best 2.5092 | lr 0.000036\n",
            "[16/30] train 2.6364 | val 2.5141 | best 2.5092 | lr 0.000038\n",
            "[17/30] train 2.6356 | val 2.5431 | best 2.5092 | lr 0.000041\n",
            "[18/30] train 2.6354 | val 2.5520 | best 2.5092 | lr 0.000043\n",
            "[19/30] train 2.6309 | val 2.5547 | best 2.5092 | lr 0.000046\n",
            "[20/30] train 2.6225 | val 2.5292 | best 2.5092 | lr 0.000048\n",
            "[21/30] train 2.6291 | val 2.5589 | best 2.5092 | lr 0.000050\n",
            "[22/30] train 2.6181 | val 2.5356 | best 2.5092 | lr 0.000053\n",
            "[23/30] train 2.6106 | val 2.5021 | best 2.5021 | lr 0.000055\n",
            "[24/30] train 2.6136 | val 2.5187 | best 2.5021 | lr 0.000058\n",
            "[25/30] train 2.6235 | val 2.5440 | best 2.5021 | lr 0.000060\n",
            "[26/30] train 2.5974 | val 2.5346 | best 2.5021 | lr 0.000062\n",
            "[27/30] train 2.5911 | val 2.5389 | best 2.5021 | lr 0.000065\n",
            "[28/30] train 2.5802 | val 2.5183 | best 2.5021 | lr 0.000067\n",
            "[29/30] train 2.5718 | val 2.5144 | best 2.5021 | lr 0.000070\n",
            "[30/30] train 2.5585 | val 2.4956 | best 2.4956 | lr 0.000072\n",
            "[SWA] val 2.5102\n",
            "Saved → /content/drive/MyDrive/DL/beethoven_dataset/sample_beethoven_long_v2.mid\n",
            "Best val loss (EMA): 2.4956124226252236 | approx PPL: 12.12915941611489\n"
          ]
        }
      ],
      "source": [
        "# ====== 0) 준비 ======\n",
        "import math, torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.amp import GradScaler, autocast  # AMP 최신 API\n",
        "from torch.optim.swa_utils import AveragedModel  # SWA\n",
        "\n",
        "VOCAB_SIZE = len(VOCAB)\n",
        "PAD_ID = VOCAB[\"[PAD]\"]\n",
        "UNK_ID = VOCAB.get(\"[UNK]\", None)\n",
        "assert UNK_ID is not None, \"[UNK] 토큰이 vocab에 필요합니다.\"\n",
        "\n",
        "# ----- (선택) 토큰 드롭아웃: VEL_/DUR_ 소량 마스킹 -----\n",
        "DROP_PROB = 0.05  # 3~7% 권장\n",
        "VEL_IDS = {tid for tok, tid in VOCAB.items() if tok.startswith(\"VEL_\")}\n",
        "DUR_IDS = {tid for tok, tid in VOCAB.items() if tok.startswith(\"DUR_\")}\n",
        "DROP_SET = VEL_IDS | DUR_IDS\n",
        "\n",
        "def token_dropout(batch_ids, drop_prob=DROP_PROB):\n",
        "    # batch_ids: LongTensor [B, T]\n",
        "    if drop_prob <= 0 or not DROP_SET:\n",
        "        return batch_ids\n",
        "    dev = batch_ids.device\n",
        "    drop_ids = torch.tensor(list(DROP_SET), device=dev)\n",
        "    mask = torch.rand_like(batch_ids.float()) < drop_prob\n",
        "    sel = mask & torch.isin(batch_ids, drop_ids)\n",
        "    return batch_ids.masked_fill(sel, PAD_ID)\n",
        "\n",
        "# ====== EMA 유틸 ======\n",
        "class EMA:\n",
        "    def __init__(self, model, decay=0.999):\n",
        "        self.decay = decay\n",
        "        self.shadow = {k: p.detach().clone() for k, p in model.state_dict().items()}\n",
        "        self.backup = None\n",
        "    @torch.no_grad()\n",
        "    def update(self, model):\n",
        "        for k, p in model.state_dict().items():\n",
        "            self.shadow[k].mul_(self.decay).add_(p.detach(), alpha=1 - self.decay)\n",
        "    def store(self, model):\n",
        "        self.backup = {k: p.detach().clone() for k, p in model.state_dict().items()}\n",
        "    def copy_to(self, model):\n",
        "        model.load_state_dict(self.shadow, strict=False)\n",
        "    def restore(self, model):\n",
        "        if self.backup is not None:\n",
        "            model.load_state_dict(self.backup, strict=False)\n",
        "            self.backup = None\n",
        "\n",
        "ema = EMA(model, decay=0.999)\n",
        "\n",
        "# ====== 1) 손실, 옵티마이저, 스케줄러 ======\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    ignore_index=PAD_ID,\n",
        "    label_smoothing=0.05,\n",
        ")\n",
        "\n",
        "opt = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=3e-4,\n",
        "    betas=(0.9, 0.95),\n",
        "    weight_decay=0.03,\n",
        ")\n",
        "\n",
        "EPOCHS = 30\n",
        "warmup_steps = 1000\n",
        "total_steps  = max(1, len(train_dl) * EPOCHS)\n",
        "\n",
        "def lr_lambda(step):\n",
        "    if step < warmup_steps:\n",
        "        return step / max(1, warmup_steps)\n",
        "    progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
        "    progress = min(1.0, max(0.0, progress))\n",
        "    return 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "sched = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
        "\n",
        "use_cuda_amp = (DEVICE.type == 'cuda')\n",
        "scaler = GradScaler('cuda' if use_cuda_amp else 'cpu')\n",
        "\n",
        "# ====== R-Drop 설정 ======\n",
        "kl_factor = 1.0  # 0.5~2.0 사이 탐색 추천\n",
        "\n",
        "def sym_kl(logits1, logits2, mask=None):\n",
        "    # logits: [B, T, V]; mask: [B, T] (1=valid, 0=ignore)\n",
        "    p = F.log_softmax(logits1, dim=-1)\n",
        "    q = F.log_softmax(logits2, dim=-1)\n",
        "    p_exp = p.exp(); q_exp = q.exp()\n",
        "    kl = (p_exp * (p - q)).sum(-1) + (q_exp * (q - p)).sum(-1)  # [B, T]\n",
        "    if mask is not None:\n",
        "        kl = kl * mask\n",
        "        denom = mask.sum().clamp_min(1)\n",
        "        return kl.sum() / denom\n",
        "    return kl.mean()\n",
        "\n",
        "# ====== 2) 학습/검증 함수 (ACC=2 + R-Drop + TokenDropout) ======\n",
        "ACC = 2  # 유효 배치 x2\n",
        "\n",
        "def run_epoch(dl, train=True, grad_clip=1.0):\n",
        "    model.train(train)\n",
        "    total, n = 0.0, 0\n",
        "    if train:\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "    for step, (x, y) in enumerate(dl, 1):\n",
        "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "        if train:\n",
        "            x = token_dropout(x)  # 입력 노이즈(소량)로 강건성 ↑\n",
        "\n",
        "        with torch.set_grad_enabled(train):\n",
        "            if train:\n",
        "                with autocast('cuda' if use_cuda_amp else 'cpu'):\n",
        "                    # R-Drop: dropout 활성 상태에서 두 번 forward\n",
        "                    logits1 = model(x)\n",
        "                    logits2 = model(x)\n",
        "                    ce1 = criterion(logits1.view(-1, VOCAB_SIZE), y.view(-1))\n",
        "                    ce2 = criterion(logits2.view(-1, VOCAB_SIZE), y.view(-1))\n",
        "                    y_mask = (y != PAD_ID).float()\n",
        "                    kl = sym_kl(logits1, logits2, y_mask)\n",
        "                    loss = 0.5*(ce1+ce2) + kl_factor*kl\n",
        "                    if ACC > 1:\n",
        "                        loss = loss / ACC\n",
        "            else:\n",
        "                with autocast('cuda' if use_cuda_amp else 'cpu'):\n",
        "                    logits = model(x)\n",
        "                    loss = criterion(logits.view(-1, VOCAB_SIZE), y.view(-1))\n",
        "\n",
        "        if train:\n",
        "            scaler.scale(loss).backward()\n",
        "            if step % ACC == 0:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "                scaler.step(opt)\n",
        "                scaler.update()\n",
        "                sched.step()\n",
        "                opt.zero_grad(set_to_none=True)\n",
        "                ema.update(model)\n",
        "\n",
        "        total += loss.item() * (ACC if train and ACC > 1 else 1.0)\n",
        "        n += 1\n",
        "    return total / max(1, n)\n",
        "\n",
        "# ====== 3) SWA(막판 평균) + EMA 검증 + 베스트 저장 ======\n",
        "best_val = float('inf')\n",
        "ckpt_path = SPLIT_ROOT / \"minigpt_best.pt\"\n",
        "\n",
        "swa_start_epoch = max(5, EPOCHS - 5)  # 마지막 5에폭 평균\n",
        "swa_model = AveragedModel(model)\n",
        "\n",
        "for ep in range(1, EPOCHS+1):\n",
        "    tr = run_epoch(train_dl, train=True)\n",
        "\n",
        "    # SWA 평균 누적(에폭 단위)\n",
        "    if ep >= swa_start_epoch:\n",
        "        swa_model.update_parameters(model)\n",
        "\n",
        "    # EMA 가중치로 검증\n",
        "    ema.store(model); ema.copy_to(model)\n",
        "    vl = run_epoch(val_dl, train=False)\n",
        "    ema.restore(model)\n",
        "\n",
        "    if vl < best_val:\n",
        "        best_val = vl\n",
        "        torch.save(model.state_dict(), str(ckpt_path))\n",
        "    print(f\"[{ep}/{EPOCHS}] train {tr:.4f} | val {vl:.4f} | best {best_val:.4f} | lr {sched.get_last_lr()[0]:.6f}\")\n",
        "\n",
        "# ====== 4) 베스트 가중치 로드(안전) + (선택) SWA 평가/저장 ======\n",
        "_ = model.load_state_dict(torch.load(str(ckpt_path), map_location=DEVICE))\n",
        "\n",
        "# SWA 가중치로도 한 번 평가해보고, 더 좋으면 SWA로 저장\n",
        "try:\n",
        "    ema.store(model)\n",
        "    model.load_state_dict(swa_model.state_dict(), strict=False)\n",
        "    vl_swa = run_epoch(val_dl, train=False)\n",
        "    print(f\"[SWA] val {vl_swa:.4f}\")\n",
        "    if vl_swa < best_val:\n",
        "        torch.save(model.state_dict(), str(SPLIT_ROOT / \"minigpt_best_swa.pt\"))\n",
        "        print(\"SWA checkpoint saved (better than EMA-best).\")\n",
        "    ema.restore(model)\n",
        "except Exception as e:\n",
        "    print(\"SWA eval skipped:\", e)\n",
        "\n",
        "# ====== 5) 기본 generate (윈도우 자동 crop + 경량 반복 페널티/온도 스케줄) ======\n",
        "def temp_schedule(step, t_max, t0=0.9, t1=0.8):\n",
        "    a = min(1.0, max(0.0, step / max(1, t_max)))\n",
        "    return t0*(1-a) + t1*a\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(prompt_tokens, max_new=700, top_k=50, top_p=0.95, temp=0.9, rep_penalty=1.05):\n",
        "    model.eval()\n",
        "    pos_len = getattr(getattr(model, 'pos', None), 'shape', [1, 100000, 0])[1]\n",
        "    ids = torch.tensor([[VOCAB.get(t, UNK_ID) for t in prompt_tokens]], device=DEVICE)\n",
        "    for step in range(max_new):\n",
        "        ids_win = ids[:, -pos_len:] if ids.size(1) > pos_len else ids\n",
        "        cur_temp = temp_schedule(step, max_new, t0=temp, t1=max(0.6, temp-0.1))\n",
        "        logits = model(ids_win)[:, -1, :] / max(cur_temp, 1e-6)\n",
        "        probs = torch.softmax(logits, dim=-1)[0]\n",
        "\n",
        "        # 미세 반복 페널티(직전 토큰만 약하게)\n",
        "        last_tok = ids[0, -1]\n",
        "        probs[last_tok] = probs[last_tok] / rep_penalty\n",
        "\n",
        "        if top_k > 0:\n",
        "            topk = torch.topk(probs, top_k)\n",
        "            mask = torch.ones_like(probs, dtype=torch.bool); mask[topk.indices] = False\n",
        "            probs = probs.masked_fill(mask, 0)\n",
        "        if top_p < 1.0:\n",
        "            sprob, sidx = torch.sort(probs, descending=True)\n",
        "            keep = torch.cumsum(sprob, dim=-1) <= top_p\n",
        "            keep[0] = True\n",
        "            mask = torch.ones_like(probs, dtype=torch.bool); mask[sidx[keep]] = False\n",
        "            probs = probs.masked_fill(mask, 0)\n",
        "        probs = probs / probs.sum()\n",
        "\n",
        "        nxt = torch.multinomial(probs, 1)\n",
        "        ids = torch.cat([ids, nxt.view(1,1)], dim=1)\n",
        "        if nxt.item() == VOCAB[\"[EOS]\"]:\n",
        "            break\n",
        "    return [IVOCAB[i.item()] for i in ids[0]]\n",
        "\n",
        "# ====== 6) 길게 생성: 청크 스티칭(모델 수정 없음) ======\n",
        "def stitch_generate(prompt_tokens, total_new=512, chunk_new=700, context=480,\n",
        "                    top_k=50, top_p=0.95, temp=0.9, stop_on_eos=False):\n",
        "    all_tokens = list(prompt_tokens); made = 0\n",
        "    while made < total_new:\n",
        "        this_new = min(chunk_new, total_new - made)\n",
        "        cur_prompt = all_tokens[-context:] if len(all_tokens) > context else all_tokens\n",
        "        chunk = generate(cur_prompt, max_new=this_new, top_k=top_k, top_p=top_p, temp=temp)\n",
        "        new_part = chunk[len(cur_prompt):] if len(chunk) > len(cur_prompt) else []\n",
        "        if stop_on_eos and (\"[EOS]\" in new_part):\n",
        "            eos_idx = new_part.index(\"[EOS]\"); all_tokens += new_part[:eos_idx]; break\n",
        "        all_tokens += new_part; made += len(new_part)\n",
        "        if len(new_part) == 0: break\n",
        "    return all_tokens\n",
        "\n",
        "# ====== 7) 프롬프트 설정 & 길게 생성 & 저장 ======\n",
        "prompt = [\"[BOS]\",\"COMPOSER_Beethoven\",\"PERIOD_Middle\",\"GENRE_Sonata\",\"KEY_Cmin\",\n",
        "          \"TSig_4_4\",\"TEMPO_112\",\"BAR\",\"POS_0\",\"BAR\",\"POS_0\",\"BAR\",\"POS_0\"]\n",
        "\n",
        "tokens_long = stitch_generate(\n",
        "    prompt_tokens=prompt,\n",
        "    total_new=512,   # 길이는 유지\n",
        "    chunk_new=700,\n",
        "    context=480,\n",
        "    top_k=50, top_p=0.95, temp=0.9,\n",
        "    stop_on_eos=False\n",
        ")\n",
        "\n",
        "out_mid_long = SPLIT_ROOT / \"sample_beethoven_long_v2.mid\"\n",
        "detokenize_to_midi(tokens_long, out_mid_long)\n",
        "print(\"Saved →\", out_mid_long)\n",
        "\n",
        "# ====== 8) (선택) 퍼플렉서티로 상태 확인 ======\n",
        "print(\"Best val loss (EMA):\", best_val, \"| approx PPL:\", math.exp(best_val))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIFbj29yTRO8",
        "outputId": "79b3f3d5-b915-4cbc-f740-b153f3617c83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  fluid-soundfont-gm libevdev2 libfluidsynth3 libgudev-1.0-0 libinput-bin\n",
            "  libinput10 libinstpatch-1.0-2 libmd4c0 libmtdev1 libqt5core5a libqt5dbus5\n",
            "  libqt5gui5 libqt5network5 libqt5svg5 libqt5widgets5 libwacom-bin\n",
            "  libwacom-common libwacom9 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
            "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1\n",
            "  libxkbcommon-x11-0 qsynth qt5-gtk-platformtheme qttranslations5-l10n\n",
            "  timgm6mb-soundfont\n",
            "Suggested packages:\n",
            "  fluid-soundfont-gs qt5-image-formats-plugins qtwayland5 jackd\n",
            "The following NEW packages will be installed:\n",
            "  fluid-soundfont-gm fluidsynth libevdev2 libfluidsynth3 libgudev-1.0-0\n",
            "  libinput-bin libinput10 libinstpatch-1.0-2 libmd4c0 libmtdev1 libqt5core5a\n",
            "  libqt5dbus5 libqt5gui5 libqt5network5 libqt5svg5 libqt5widgets5 libwacom-bin\n",
            "  libwacom-common libwacom9 libxcb-icccm4 libxcb-image0 libxcb-keysyms1\n",
            "  libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1\n",
            "  libxkbcommon-x11-0 qsynth qt5-gtk-platformtheme qttranslations5-l10n\n",
            "  timgm6mb-soundfont\n",
            "0 upgraded, 32 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 148 MB of archives.\n",
            "After this operation, 207 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5core5a amd64 5.15.3+dfsg-2ubuntu0.2 [2,006 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libevdev2 amd64 1.12.1+dfsg-1 [39.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libmtdev1 amd64 1.1.6-1build4 [14.5 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgudev-1.0-0 amd64 1:237-2build1 [16.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom-common all 2.2.0-1 [54.3 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom9 amd64 2.2.0-1 [22.0 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libinput-bin amd64 1.20.0-1ubuntu0.3 [19.9 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libinput10 amd64 1.20.0-1ubuntu0.3 [131 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmd4c0 amd64 0.4.8-1 [42.0 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5dbus5 amd64 5.15.3+dfsg-2ubuntu0.2 [222 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5network5 amd64 5.15.3+dfsg-2ubuntu0.2 [731 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-icccm4 amd64 0.4.1-1.1build2 [11.5 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-util1 amd64 0.4.0-1build2 [11.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-image0 amd64 0.4.0-2 [11.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-keysyms1 amd64 0.4.0-1build3 [8,746 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-render-util0 amd64 0.3.9-1build3 [10.3 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinerama0 amd64 1.14-3ubuntu3 [5,414 B]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xinput0 amd64 1.14-3ubuntu3 [34.3 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcb-xkb1 amd64 1.14-3ubuntu3 [32.8 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-x11-0 amd64 1.4.0-1 [14.4 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5gui5 amd64 5.15.3+dfsg-2ubuntu0.2 [3,722 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libqt5widgets5 amd64 5.15.3+dfsg-2ubuntu0.2 [2,561 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libqt5svg5 amd64 5.15.3-1 [149 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fluid-soundfont-gm all 3.1-5.3 [130 MB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libinstpatch-1.0-2 amd64 1.1.6-1 [240 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/universe amd64 timgm6mb-soundfont all 1.3-5 [5,427 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfluidsynth3 amd64 2.2.5-1 [246 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy/universe amd64 fluidsynth amd64 2.2.5-1 [27.4 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libwacom-bin amd64 2.2.0-1 [13.6 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qsynth amd64 0.9.6-1 [305 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 qt5-gtk-platformtheme amd64 5.15.3+dfsg-2ubuntu0.2 [130 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/universe amd64 qttranslations5-l10n all 5.15.3-1 [1,983 kB]\n",
            "Fetched 148 MB in 10s (14.7 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package libqt5core5a:amd64.\n",
            "(Reading database ... 126374 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libqt5core5a_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libevdev2:amd64.\n",
            "Preparing to unpack .../01-libevdev2_1.12.1+dfsg-1_amd64.deb ...\n",
            "Unpacking libevdev2:amd64 (1.12.1+dfsg-1) ...\n",
            "Selecting previously unselected package libmtdev1:amd64.\n",
            "Preparing to unpack .../02-libmtdev1_1.1.6-1build4_amd64.deb ...\n",
            "Unpacking libmtdev1:amd64 (1.1.6-1build4) ...\n",
            "Selecting previously unselected package libgudev-1.0-0:amd64.\n",
            "Preparing to unpack .../03-libgudev-1.0-0_1%3a237-2build1_amd64.deb ...\n",
            "Unpacking libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Selecting previously unselected package libwacom-common.\n",
            "Preparing to unpack .../04-libwacom-common_2.2.0-1_all.deb ...\n",
            "Unpacking libwacom-common (2.2.0-1) ...\n",
            "Selecting previously unselected package libwacom9:amd64.\n",
            "Preparing to unpack .../05-libwacom9_2.2.0-1_amd64.deb ...\n",
            "Unpacking libwacom9:amd64 (2.2.0-1) ...\n",
            "Selecting previously unselected package libinput-bin.\n",
            "Preparing to unpack .../06-libinput-bin_1.20.0-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking libinput-bin (1.20.0-1ubuntu0.3) ...\n",
            "Selecting previously unselected package libinput10:amd64.\n",
            "Preparing to unpack .../07-libinput10_1.20.0-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking libinput10:amd64 (1.20.0-1ubuntu0.3) ...\n",
            "Selecting previously unselected package libmd4c0:amd64.\n",
            "Preparing to unpack .../08-libmd4c0_0.4.8-1_amd64.deb ...\n",
            "Unpacking libmd4c0:amd64 (0.4.8-1) ...\n",
            "Selecting previously unselected package libqt5dbus5:amd64.\n",
            "Preparing to unpack .../09-libqt5dbus5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5network5:amd64.\n",
            "Preparing to unpack .../10-libqt5network5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libxcb-icccm4:amd64.\n",
            "Preparing to unpack .../11-libxcb-icccm4_0.4.1-1.1build2_amd64.deb ...\n",
            "Unpacking libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Selecting previously unselected package libxcb-util1:amd64.\n",
            "Preparing to unpack .../12-libxcb-util1_0.4.0-1build2_amd64.deb ...\n",
            "Unpacking libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Selecting previously unselected package libxcb-image0:amd64.\n",
            "Preparing to unpack .../13-libxcb-image0_0.4.0-2_amd64.deb ...\n",
            "Unpacking libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Selecting previously unselected package libxcb-keysyms1:amd64.\n",
            "Preparing to unpack .../14-libxcb-keysyms1_0.4.0-1build3_amd64.deb ...\n",
            "Unpacking libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Selecting previously unselected package libxcb-render-util0:amd64.\n",
            "Preparing to unpack .../15-libxcb-render-util0_0.3.9-1build3_amd64.deb ...\n",
            "Unpacking libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Selecting previously unselected package libxcb-xinerama0:amd64.\n",
            "Preparing to unpack .../16-libxcb-xinerama0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-xinput0:amd64.\n",
            "Preparing to unpack .../17-libxcb-xinput0_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxcb-xkb1:amd64.\n",
            "Preparing to unpack .../18-libxcb-xkb1_1.14-3ubuntu3_amd64.deb ...\n",
            "Unpacking libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Selecting previously unselected package libxkbcommon-x11-0:amd64.\n",
            "Preparing to unpack .../19-libxkbcommon-x11-0_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libqt5gui5:amd64.\n",
            "Preparing to unpack .../20-libqt5gui5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5widgets5:amd64.\n",
            "Preparing to unpack .../21-libqt5widgets5_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package libqt5svg5:amd64.\n",
            "Preparing to unpack .../22-libqt5svg5_5.15.3-1_amd64.deb ...\n",
            "Unpacking libqt5svg5:amd64 (5.15.3-1) ...\n",
            "Selecting previously unselected package fluid-soundfont-gm.\n",
            "Preparing to unpack .../23-fluid-soundfont-gm_3.1-5.3_all.deb ...\n",
            "Unpacking fluid-soundfont-gm (3.1-5.3) ...\n",
            "Selecting previously unselected package libinstpatch-1.0-2:amd64.\n",
            "Preparing to unpack .../24-libinstpatch-1.0-2_1.1.6-1_amd64.deb ...\n",
            "Unpacking libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
            "Selecting previously unselected package timgm6mb-soundfont.\n",
            "Preparing to unpack .../25-timgm6mb-soundfont_1.3-5_all.deb ...\n",
            "Unpacking timgm6mb-soundfont (1.3-5) ...\n",
            "Selecting previously unselected package libfluidsynth3:amd64.\n",
            "Preparing to unpack .../26-libfluidsynth3_2.2.5-1_amd64.deb ...\n",
            "Unpacking libfluidsynth3:amd64 (2.2.5-1) ...\n",
            "Selecting previously unselected package fluidsynth.\n",
            "Preparing to unpack .../27-fluidsynth_2.2.5-1_amd64.deb ...\n",
            "Unpacking fluidsynth (2.2.5-1) ...\n",
            "Selecting previously unselected package libwacom-bin.\n",
            "Preparing to unpack .../28-libwacom-bin_2.2.0-1_amd64.deb ...\n",
            "Unpacking libwacom-bin (2.2.0-1) ...\n",
            "Selecting previously unselected package qsynth.\n",
            "Preparing to unpack .../29-qsynth_0.9.6-1_amd64.deb ...\n",
            "Unpacking qsynth (0.9.6-1) ...\n",
            "Selecting previously unselected package qt5-gtk-platformtheme:amd64.\n",
            "Preparing to unpack .../30-qt5-gtk-platformtheme_5.15.3+dfsg-2ubuntu0.2_amd64.deb ...\n",
            "Unpacking qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Selecting previously unselected package qttranslations5-l10n.\n",
            "Preparing to unpack .../31-qttranslations5-l10n_5.15.3-1_all.deb ...\n",
            "Unpacking qttranslations5-l10n (5.15.3-1) ...\n",
            "Setting up libxcb-xinput0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-keysyms1:amd64 (0.4.0-1build3) ...\n",
            "Setting up libxcb-render-util0:amd64 (0.3.9-1build3) ...\n",
            "Setting up libxcb-icccm4:amd64 (0.4.1-1.1build2) ...\n",
            "Setting up libxcb-util1:amd64 (0.4.0-1build2) ...\n",
            "Setting up libxcb-xkb1:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up libxcb-image0:amd64 (0.4.0-2) ...\n",
            "Setting up libxcb-xinerama0:amd64 (1.14-3ubuntu3) ...\n",
            "Setting up qttranslations5-l10n (5.15.3-1) ...\n",
            "Setting up libxkbcommon-x11-0:amd64 (1.4.0-1) ...\n",
            "Setting up libqt5core5a:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libmtdev1:amd64 (1.1.6-1build4) ...\n",
            "Setting up libqt5dbus5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libmd4c0:amd64 (0.4.8-1) ...\n",
            "Setting up fluid-soundfont-gm (3.1-5.3) ...\n",
            "update-alternatives: using /usr/share/sounds/sf2/FluidR3_GM.sf2 to provide /usr/share/sounds/sf2/default-GM.sf2 (default-GM.sf2) in auto mode\n",
            "update-alternatives: using /usr/share/sounds/sf2/FluidR3_GM.sf2 to provide /usr/share/sounds/sf3/default-GM.sf3 (default-GM.sf3) in auto mode\n",
            "Setting up timgm6mb-soundfont (1.3-5) ...\n",
            "Setting up libevdev2:amd64 (1.12.1+dfsg-1) ...\n",
            "Setting up libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
            "Setting up libgudev-1.0-0:amd64 (1:237-2build1) ...\n",
            "Setting up libfluidsynth3:amd64 (2.2.5-1) ...\n",
            "Setting up libwacom-common (2.2.0-1) ...\n",
            "Setting up libwacom9:amd64 (2.2.0-1) ...\n",
            "Setting up libqt5network5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libinput-bin (1.20.0-1ubuntu0.3) ...\n",
            "Setting up fluidsynth (2.2.5-1) ...\n",
            "Created symlink /etc/systemd/user/default.target.wants/fluidsynth.service → /usr/lib/systemd/user/fluidsynth.service.\n",
            "Setting up libwacom-bin (2.2.0-1) ...\n",
            "Setting up libinput10:amd64 (1.20.0-1ubuntu0.3) ...\n",
            "Setting up libqt5gui5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libqt5widgets5:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up qt5-gtk-platformtheme:amd64 (5.15.3+dfsg-2ubuntu0.2) ...\n",
            "Setting up libqt5svg5:amd64 (5.15.3-1) ...\n",
            "Setting up qsynth (0.9.6-1) ...\n",
            "Processing triggers for mailcap (3.70+nmu1ubuntu1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Requirement already satisfied: pyfluidsynth in /usr/local/lib/python3.12/dist-packages (1.3.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pyfluidsynth) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!apt -q install -y fluidsynth\n",
        "!pip install pyfluidsynth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pJYE9E3T3rL",
        "outputId": "710a129f-4d72-4f18-d1fe-86213c3f2cf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyFluidSynth==1.3.4 in /usr/local/lib/python3.12/dist-packages (1.3.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pyFluidSynth==1.3.4) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyFluidSynth==1.3.4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVm3SK1wkHaa"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nApGl8Ek05d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
